{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fb64b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:32.335240Z",
     "iopub.status.busy": "2023-12-13T19:02:32.334270Z",
     "iopub.status.idle": "2023-12-13T19:02:38.086497Z",
     "shell.execute_reply": "2023-12-13T19:02:38.085537Z"
    },
    "papermill": {
     "duration": 5.762141,
     "end_time": "2023-12-13T19:02:38.088918",
     "exception": false,
     "start_time": "2023-12-13T19:02:32.326777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import torch\n",
    "import unicodedata\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57f4f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.103550Z",
     "iopub.status.busy": "2023-12-13T19:02:38.103027Z",
     "iopub.status.idle": "2023-12-13T19:02:38.114889Z",
     "shell.execute_reply": "2023-12-13T19:02:38.113854Z"
    },
    "papermill": {
     "duration": 0.021602,
     "end_time": "2023-12-13T19:02:38.116957",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.095355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Utility class created to thoroughly document the outcomes of each experiment.\n",
    "        \"\"\"\n",
    "        currentDirectory = os.getcwd()\n",
    "        directories = []\n",
    "\n",
    "        try:\n",
    "            os.mkdir(f\"{currentDirectory}\\\\{name}\")\n",
    "        except OSError as osError:\n",
    "            print(f\"Logger: {osError}\")\n",
    "\n",
    "        currentDirectory = f\"{currentDirectory}\\\\{name}\"\n",
    "\n",
    "        for dirName in os.listdir(currentDirectory):\n",
    "            if \"EXPERIMENT\" in dirName:\n",
    "                directories.append(int(dirName[dirName.index(\"-\") + 1:]))\n",
    "\n",
    "        if len(directories) == 0:\n",
    "            self.__directory = f\"{currentDirectory}\\\\EXPERIMENT-0\"\n",
    "            os.mkdir(self.__directory)\n",
    "\n",
    "        else:\n",
    "            directories.sort()\n",
    "            latestDirectory = directories[-1]\n",
    "            self.__directory = f\"{currentDirectory}\\\\EXPERIMENT-{latestDirectory + 1}\"\n",
    "            os.mkdir(self.__directory)\n",
    "\n",
    "        self.__log = open(f\"{self.__directory}\\\\LOG.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    def log(self, toLog, newLine=True, title=None):\n",
    "        if newLine:\n",
    "            toLog += \"\\n\"\n",
    "        if title is not None:\n",
    "            self.__log.write(f\"{title}\\n\")\n",
    "        self.__log.write(toLog)\n",
    "        self.__log.flush()\n",
    "\n",
    "    def line(self):\n",
    "        self.__log.write(\"\\n\\n\")\n",
    "        self.__log.flush()\n",
    "\n",
    "    def directory(self):\n",
    "        return self.__directory\n",
    "\n",
    "    def close(self):\n",
    "        self.__log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d807d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.131327Z",
     "iopub.status.busy": "2023-12-13T19:02:38.130626Z",
     "iopub.status.idle": "2023-12-13T19:02:38.139191Z",
     "shell.execute_reply": "2023-12-13T19:02:38.138110Z"
    },
    "papermill": {
     "duration": 0.01807,
     "end_time": "2023-12-13T19:02:38.141338",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.123268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to the word embeddings.\n",
    "        \"\"\"\n",
    "        self.__embeddings = dict()\n",
    "        self.__dimensions = -1\n",
    "        file = open(path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        for embedding in file:\n",
    "            vector = []\n",
    "\n",
    "            tokens = embedding.replace(\"\\n\", \"\").split()\n",
    "\n",
    "            for i in range(1, len(tokens)):\n",
    "                vector.append(float(tokens[i]))\n",
    "\n",
    "            self.__embeddings[tokens[0]] = vector\n",
    "\n",
    "            if self.__dimensions == -1:\n",
    "                self.__dimensions = len(tokens) - 1\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.__embeddings.keys()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item in self.__embeddings.keys():\n",
    "            return self.__embeddings[item]\n",
    "        else:\n",
    "            return [0.0 for _ in range(self.__dimensions)]\n",
    "\n",
    "    def dimensions(self):\n",
    "        return self.__dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c50462e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.155991Z",
     "iopub.status.busy": "2023-12-13T19:02:38.155189Z",
     "iopub.status.idle": "2023-12-13T19:02:38.166321Z",
     "shell.execute_reply": "2023-12-13T19:02:38.165339Z"
    },
    "papermill": {
     "duration": 0.020977,
     "end_time": "2023-12-13T19:02:38.168615",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.147638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    classToNumber = {\"POSITIVE\": 0, \"NEGATIVE\": 1, \"NEUTRAL\": 2}\n",
    "    numberToClass = {0: \"POSITIVE\", 1: \"NEGATIVE\", 2: \"NEUTRAL\"}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to a tweet.\n",
    "        \"\"\"\n",
    "        self.__text = None\n",
    "        self.__mentions = []\n",
    "        self.__links = []\n",
    "        self.__hashtags = []\n",
    "        self.__sentiment = None\n",
    "        self.__party = None\n",
    "        self.__id = None\n",
    "\n",
    "    def setID(self, id):\n",
    "        self.__id = id\n",
    "\n",
    "    def getID(self):\n",
    "        return self.__id\n",
    "\n",
    "    def addLinks(self, links):\n",
    "        for link in links:\n",
    "            self.__links.append(link)\n",
    "\n",
    "    def getLinks(self):\n",
    "        return self.__links\n",
    "\n",
    "    def getTotalLinks(self):\n",
    "        return len(self.__links)\n",
    "\n",
    "    def addHashtags(self, hashtags):\n",
    "        for hashtag in hashtags:\n",
    "            self.__hashtags.append(hashtag)\n",
    "\n",
    "    def getHashtags(self):\n",
    "        return self.__hashtags\n",
    "\n",
    "    def getTotalHashtags(self):\n",
    "        return len(self.__hashtags)\n",
    "\n",
    "    def addMentions(self, mentions):\n",
    "        for mention in mentions:\n",
    "            self.__mentions.append(mention)\n",
    "\n",
    "    def getMentions(self):\n",
    "        return self.__mentions\n",
    "\n",
    "    def getTotalMentions(self):\n",
    "        return len(self.__mentions)\n",
    "\n",
    "    def setParty(self, party):\n",
    "        self.__party = party\n",
    "\n",
    "    def getParty(self):\n",
    "        return self.__party\n",
    "\n",
    "    def setSentiment(self, sentiment):\n",
    "        self.__sentiment = sentiment\n",
    "\n",
    "    def getSentiment(self):\n",
    "        return self.__sentiment\n",
    "\n",
    "    def setText(self, text):\n",
    "        self.__text = text\n",
    "\n",
    "    def getText(self):\n",
    "        return self.__text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65536d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.183543Z",
     "iopub.status.busy": "2023-12-13T19:02:38.182855Z",
     "iopub.status.idle": "2023-12-13T19:02:38.189271Z",
     "shell.execute_reply": "2023-12-13T19:02:38.188168Z"
    },
    "papermill": {
     "duration": 0.016672,
     "end_time": "2023-12-13T19:02:38.191701",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.175029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stopwords:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to Greek stopwords.\n",
    "        \"\"\"\n",
    "        self.__stopwords = set()\n",
    "\n",
    "        stopwordFile = open(path, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for word in stopwordFile:\n",
    "            words.append(word)\n",
    "\n",
    "        for word in words:\n",
    "            stopword = word.replace(\"\\n\", \"\")\n",
    "            self.__stopwords.add(stopword)\n",
    "\n",
    "        stopwordFile.close()\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.__stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed59aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.206682Z",
     "iopub.status.busy": "2023-12-13T19:02:38.206325Z",
     "iopub.status.idle": "2023-12-13T19:02:38.254981Z",
     "shell.execute_reply": "2023-12-13T19:02:38.253894Z"
    },
    "papermill": {
     "duration": 0.059286,
     "end_time": "2023-12-13T19:02:38.257346",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.198060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the Greek stemmer based on the paper of Georgios Ntais which can be found at the following link https://people.dsv.su.se/~hercules/papers/Ntais_greek_stemmer_thesis_final.pdf\n",
    "\"\"\"\n",
    "\n",
    "__cases = dict()\n",
    "__cases[\"ΦΑΓΙΑ\"] = \"ΦΑ\"\n",
    "__cases[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"\n",
    "__cases[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"\n",
    "__cases[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"\n",
    "__cases[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"\n",
    "__cases[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"\n",
    "__cases[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"\n",
    "__cases[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"\n",
    "__cases[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"\n",
    "__cases[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"\n",
    "__cases[\"ΣΟΓΙΑ\"] = \"ΣΟ\"\n",
    "__cases[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"\n",
    "__cases[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"\n",
    "__cases[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"\n",
    "__cases[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"\n",
    "__cases[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"\n",
    "__cases[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"\n",
    "__cases[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"\n",
    "__cases[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"\n",
    "__cases[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"\n",
    "__cases[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"\n",
    "__cases[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"\n",
    "__cases[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"\n",
    "__cases[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"\n",
    "__cases[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"\n",
    "__cases[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"\n",
    "__cases[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"\n",
    "__cases[\"ΦΩΣ\"] = \"ΦΩ\"\n",
    "__cases[\"ΦΩΤΟΣ\"] = \"ΦΩ\"\n",
    "__cases[\"ΦΩΤΑ\"] = \"ΦΩ\"\n",
    "__cases[\"ΦΩΤΩΝ\"] = \"ΦΩ\"\n",
    "__cases[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"\n",
    "__cases[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"\n",
    "__cases[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"\n",
    "__cases[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"\n",
    "__cases[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"\n",
    "__cases[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"\n",
    "__cases[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"\n",
    "__cases[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\"\n",
    "__vowels = \"[ΑΕΗΙΟΥΩ]\"\n",
    "__refinedVowels = \"[ΑΕΗΙΟΩ]\"\n",
    "\n",
    "def stemWord(w: str, exceptions: dict = None):\n",
    "    stem = None\n",
    "    suffix = None\n",
    "    test1 = True\n",
    "\n",
    "    if exceptions is not None and w in exceptions.keys():\n",
    "        return exceptions[w]\n",
    "\n",
    "    if len(w) < 4:\n",
    "        return w\n",
    "\n",
    "    pattern = None\n",
    "    pattern2 = None\n",
    "    pattern3 = None\n",
    "    pattern4 = None\n",
    "\n",
    "    # Step1\n",
    "    pattern = re.compile(\n",
    "        r\"(.*)(ΦΑΓΙΑ|ΦΑΓΙΟΥ|ΦΑΓΙΩΝ|ΣΚΑΓΙΑ|ΣΚΑΓΙΟΥ|ΣΚΑΓΙΩΝ|ΟΛΟΓΙΟΥ|ΟΛΟΓΙΑ|ΟΛΟΓΙΩΝ|ΣΟΓΙΟΥ|ΣΟΓΙΑ|ΣΟΓΙΩΝ|ΤΑΤΟΓΙΑ|ΤΑΤΟΓΙΟΥ|ΤΑΤΟΓΙΩΝ|ΚΡΕΑΣ|ΚΡΕΑΤΟΣ|ΚΡΕΑΤΑ|ΚΡΕΑΤΩΝ|ΠΕΡΑΣ|ΠΕΡΑΤΟΣ|ΠΕΡΑΤΑ|ΠΕΡΑΤΩΝ|ΤΕΡΑΣ|ΤΕΡΑΤΟΣ|ΤΕΡΑΤΑ|ΤΕΡΑΤΩΝ|ΦΩΣ|ΦΩΤΟΣ|ΦΩΤΑ|ΦΩΤΩΝ|ΚΑΘΕΣΤΩΣ|ΚΑΘΕΣΤΩΤΟΣ|ΚΑΘΕΣΤΩΤΑ|ΚΑΘΕΣΤΩΤΩΝ|ΓΕΓΟΝΟΣ|ΓΕΓΟΝΟΤΟΣ|ΓΕΓΟΝΟΤΑ|ΓΕΓΟΝΟΤΩΝ)$\")\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        suffix = fp[1]\n",
    "        w = stem + __cases[suffix]\n",
    "        test1 = False\n",
    "\n",
    "    # Step 2a\n",
    "    pattern = re.compile(r\"^(.+?)(ΑΔΕΣ|ΑΔΩΝ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        reg1 = re.compile(r\"(ΟΚ|ΜΑΜ|ΜΑΝ|ΜΠΑΜΠ|ΠΑΤΕΡ|ΓΙΑΓΙ|ΝΤΑΝΤ|ΚΥΡ|ΘΕΙ|ΠΕΘΕΡ)$\")\n",
    "\n",
    "        if not reg1.match(w):\n",
    "            w = w + \"ΑΔ\"\n",
    "\n",
    "    # Step 2b\n",
    "    pattern2 = re.compile(r\"^(.+?)(ΕΔΕΣ|ΕΔΩΝ)$\")\n",
    "    if pattern2.match(w):\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        except2 = re.compile(r\"(ΟΠ|ΙΠ|ΕΜΠ|ΥΠ|ΓΗΠ|ΔΑΠ|ΚΡΑΣΠ|ΜΙΛ)$\")\n",
    "        if except2.match(w):\n",
    "            w = w + \"ΕΔ\"\n",
    "\n",
    "    # Step 2c\n",
    "    pattern3 = re.compile(r\"^(.+?)(ΟΥΔΕΣ|ΟΥΔΩΝ)$\")\n",
    "    if pattern3.match(w):\n",
    "        fp = pattern3.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        except3 = re.compile(r\"(ΑΡΚ|ΚΑΛΙΑΚ|ΠΕΤΑΛ|ΛΙΧ|ΠΛΕΞ|ΣΚ|Σ|ΦΛ|ΦΡ|ΒΕΛ|ΛΟΥΛ|ΧΝ|ΣΠ|ΤΡΑΓ|ΦΕ)$\")\n",
    "        if except3.match(w):\n",
    "            w = w + \"ΟΥΔ\"\n",
    "\n",
    "    # Step 2d\n",
    "    pattern4 = re.compile(\"^(.+?)(ΕΩΣ|ΕΩΝ)$\")\n",
    "    if pattern4.match(w):\n",
    "        fp = pattern4.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except4 = re.compile(r\"^(Θ|Δ|ΕΛ|ΓΑΛ|Ν|Π|ΙΔ|ΠΑΡ)$\")\n",
    "        if except4.match(w):\n",
    "            w = w + \"Ε\"\n",
    "\n",
    "    # Step 3\n",
    "    pattern = re.compile(r\"^(.+?)(ΙΑ|ΙΟΥ|ΙΩΝ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        pattern2 = re.compile(__vowels + \"$\")\n",
    "        test1 = False\n",
    "        if pattern2.match(w):\n",
    "            w = stem + \"Ι\"\n",
    "\n",
    "    # Step 4\n",
    "    pattern = re.compile(r\"^(.+?)(ΙΚΑ|ΙΚΟ|ΙΚΟΥ|ΙΚΩΝ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        pattern2 = re.compile(__vowels + \"$\")\n",
    "        except5 = re.compile(\n",
    "            r\"^(ΑΛ|ΑΔ|ΕΝΔ|ΑΜΑΝ|ΑΜΜΟΧΑΛ|ΗΘ|ΑΝΗΘ|ΑΝΤΙΔ|ΦΥΣ|ΒΡΩΜ|ΓΕΡ|ΕΞΩΔ|ΚΑΛΠ|ΚΑΛΛΙΝ|ΚΑΤΑΔ|ΜΟΥΛ|ΜΠΑΝ|ΜΠΑΓΙΑΤ|ΜΠΟΛ|ΜΠΟΣ|ΝΙΤ|ΞΙΚ|ΣΥΝΟΜΗΛ|ΠΕΤΣ|ΠΙΤΣ|ΠΙΚΑΝΤ|ΠΛΙΑΤΣ|ΠΟΣΤΕΛΝ|ΠΡΩΤΟΔ|ΣΕΡΤ|ΣΥΝΑΔ|ΤΣΑΜ|ΥΠΟΔ|ΦΙΛΟΝ|ΦΥΛΟΔ|ΧΑΣ)$\")\n",
    "        if except5.match(w) or pattern2.match(w):\n",
    "            w = w + \"ΙΚ\"\n",
    "\n",
    "    # step 5a\n",
    "    pattern = re.compile(r\"^(.+?)(ΑΜΕ)$\")\n",
    "    pattern2 = re.compile(r\"^(.+?)(ΑΓΑΜΕ|ΗΣΑΜΕ|ΟΥΣΑΜΕ|ΗΚΑΜΕ|ΗΘΗΚΑΜΕ)$\")\n",
    "    if w == \"ΑΓΑΜΕ\":\n",
    "        w = \"ΑΓΑΜ\"\n",
    "\n",
    "    if pattern2.match(w):\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except6 = re.compile(r\"^(ΑΝΑΠ|ΑΠΟΘ|ΑΠΟΚ|ΑΠΟΣΤ|ΒΟΥΒ|ΞΕΘ|ΟΥΛ|ΠΕΘ|ΠΙΚΡ|ΠΟΤ|ΣΙΧ|Χ)$\")\n",
    "        if except6.match(w):\n",
    "            w = w + \"ΑΜ\"\n",
    "\n",
    "    # Step 5b\n",
    "    pattern2 = re.compile(r\"^(.+?)(ΑΝΕ)$\")\n",
    "    pattern3 = re.compile(r\"^(.+?)(ΑΓΑΝΕ|ΗΣΑΝΕ|ΟΥΣΑΝΕ|ΙΟΝΤΑΝΕ|ΙΟΤΑΝΕ|ΙΟΥΝΤΑΝΕ|ΟΝΤΑΝΕ|ΟΤΑΝΕ|ΟΥΝΤΑΝΕ|ΗΚΑΝΕ|ΗΘΗΚΑΝΕ)$\")\n",
    "    if pattern3.match(w):\n",
    "        fp = pattern3.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        pattern3 = re.compile(r\"^(ΤΡ|ΤΣ)$\")\n",
    "        if pattern3.match(w):\n",
    "            w = w + \"ΑΓΑΝ\"\n",
    "\n",
    "    if pattern2.match(w):\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        pattern2 = re.compile(__refinedVowels + \"$\")\n",
    "        except7 = re.compile(\n",
    "            r\"^(ΒΕΤΕΡ|ΒΟΥΛΚ|ΒΡΑΧΜ|Γ|ΔΡΑΔΟΥΜ|Θ|ΚΑΛΠΟΥΖ|ΚΑΣΤΕΛ|ΚΟΡΜΟΡ|ΛΑΟΠΛ|ΜΩΑΜΕΘ|Μ|ΜΟΥΣΟΥΛΜ|Ν|ΟΥΛ|Π|ΠΕΛΕΚ|ΠΛ|ΠΟΛΙΣ|ΠΟΡΤΟΛ|ΣΑΡΑΚΑΤΣ|ΣΟΥΛΤ|ΤΣΑΡΛΑΤ|ΟΡΦ|ΤΣΙΓΓ|ΤΣΟΠ|ΦΩΤΟΣΤΕΦ|Χ|ΨΥΧΟΠΛ|ΑΓ|ΟΡΦ|ΓΑΛ|ΓΕΡ|ΔΕΚ|ΔΙΠΛ|ΑΜΕΡΙΚΑΝ|ΟΥΡ|ΠΙΘ|ΠΟΥΡΙΤ|Σ|ΖΩΝΤ|ΙΚ|ΚΑΣΤ|ΚΟΠ|ΛΙΧ|ΛΟΥΘΗΡ|ΜΑΙΝΤ|ΜΕΛ|ΣΙΓ|ΣΠ|ΣΤΕΓ|ΤΡΑΓ|ΤΣΑΓ|Φ|ΕΡ|ΑΔΑΠ|ΑΘΙΓΓ|ΑΜΗΧ|ΑΝΙΚ|ΑΝΟΡΓ|ΑΠΗΓ|ΑΠΙΘ|ΑΤΣΙΓΓ|ΒΑΣ|ΒΑΣΚ|ΒΑΘΥΓΑΛ|ΒΙΟΜΗΧ|ΒΡΑΧΥΚ|ΔΙΑΤ|ΔΙΑΦ|ΕΝΟΡΓ|ΘΥΣ|ΚΑΠΝΟΒΙΟΜΗΧ|ΚΑΤΑΓΑΛ|ΚΛΙΒ|ΚΟΙΛΑΡΦ|ΛΙΒ|ΜΕΓΛΟΒΙΟΜΗΧ|ΜΙΚΡΟΒΙΟΜΗΧ|ΝΤΑΒ|ΞΗΡΟΚΛΙΒ|ΟΛΙΓΟΔΑΜ|ΟΛΟΓΑΛ|ΠΕΝΤΑΡΦ|ΠΕΡΗΦ|ΠΕΡΙΤΡ|ΠΛΑΤ|ΠΟΛΥΔΑΠ|ΠΟΛΥΜΗΧ|ΣΤΕΦ|ΤΑΒ|ΤΕΤ|ΥΠΕΡΗΦ|ΥΠΟΚΟΠ|ΧΑΜΗΛΟΔΑΠ|ΨΗΛΟΤΑΒ)$\")\n",
    "        if (pattern2.match(w)) or (except7.match(w)):\n",
    "            w = w + \"ΑΝ\"\n",
    "\n",
    "    # //Step 5c\n",
    "    pattern3 = re.compile(r\"^(.+?)(ΕΤΕ)$\")\n",
    "    pattern4 = re.compile(r\"^(.+?)(ΗΣΕΤΕ)$\")\n",
    "    if pattern4.match(w):\n",
    "        fp = pattern4.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "\n",
    "    if pattern3.match(w):\n",
    "        fp = pattern3.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        pattern3 = re.compile(__refinedVowels + \"$\")\n",
    "        except8 = re.compile(\n",
    "            r\"(ΟΔ|ΑΙΡ|ΦΟΡ|ΤΑΘ|ΔΙΑΘ|ΣΧ|ΕΝΔ|ΕΥΡ|ΤΙΘ|ΥΠΕΡΘ|ΡΑΘ|ΕΝΘ|ΡΟΘ|ΣΘ|ΠΥΡ|ΑΙΝ|ΣΥΝΔ|ΣΥΝ|ΣΥΝΘ|ΧΩΡ|ΠΟΝ|ΒΡ|ΚΑΘ|ΕΥΘ|ΕΚΘ|ΝΕΤ|ΡΟΝ|ΑΡΚ|ΒΑΡ|ΒΟΛ|ΩΦΕΛ)$\")\n",
    "        except9 = re.compile(\n",
    "            r\"^(ΑΒΑΡ|ΒΕΝ|ΕΝΑΡ|ΑΒΡ|ΑΔ|ΑΘ|ΑΝ|ΑΠΛ|ΒΑΡΟΝ|ΝΤΡ|ΣΚ|ΚΟΠ|ΜΠΟΡ|ΝΙΦ|ΠΑΓ|ΠΑΡΑΚΑΛ|ΣΕΡΠ|ΣΚΕΛ|ΣΥΡΦ|ΤΟΚ|Υ|Δ|ΕΜ|ΘΑΡΡ|Θ)$\")\n",
    "        if (pattern3.match(w)) or (except8.match(w)) or (except9.match(w)):\n",
    "            w = w + \"ΕΤ\"\n",
    "\n",
    "    # Step 5d\n",
    "    pattern = re.compile(r\"^(.+?)(ΟΝΤΑΣ|ΩΝΤΑΣ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except10 = re.compile(r\"^(ΑΡΧ)$\")\n",
    "        except11 = re.compile(r\"(ΚΡΕ)$\")\n",
    "        if except10.match(w):\n",
    "            w = w + \"ΟΝΤ\"\n",
    "        if except11.match(w):\n",
    "            w = w + \"ΩΝΤ\"\n",
    "\n",
    "    # Step 5e\n",
    "    pattern = re.compile(r\"^(.+?)(ΟΜΑΣΤΕ|ΙΟΜΑΣΤΕ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except11 = re.compile(\"^(ΟΝ)$\")\n",
    "        if except11.match(w):\n",
    "            w = w + \"ΟΜΑΣΤ\"\n",
    "\n",
    "    # Step 5f\n",
    "    pattern = re.compile(r\"^(.+?)(ΕΣΤΕ)$\")\n",
    "    pattern2 = re.compile(r\"^(.+?)(ΙΕΣΤΕ)$\")\n",
    "    if pattern2.match(w):\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        pattern2 = re.compile(r\"^(Π|ΑΠ|ΣΥΜΠ|ΑΣΥΜΠ|ΑΚΑΤΑΠ|ΑΜΕΤΑΜΦ)$\")\n",
    "        if pattern2.match(w):\n",
    "            w = w + \"ΙΕΣΤ\"\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except12 = re.compile(r\"^(ΑΛ|ΑΡ|ΕΚΤΕΛ|Ζ|Μ|Ξ|ΠΑΡΑΚΑΛ|ΑΡ|ΠΡΟ|ΝΙΣ)$\")\n",
    "        if except12.match(w):\n",
    "            w = w + \"ΕΣΤ\"\n",
    "\n",
    "    # Step 5g\n",
    "    pattern = re.compile(r\"^(.+?)(ΗΚΑ|ΗΚΕΣ|ΗΚΕ)$\")\n",
    "    pattern2 = re.compile(r\"^(.+?)(ΗΘΗΚΑ|ΗΘΗΚΕΣ|ΗΘΗΚΕ)$\")\n",
    "    if pattern2.match(w):\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except13 = re.compile(r\"(ΣΚΩΛ|ΣΚΟΥΛ|ΝΑΡΘ|ΣΦ|ΟΘ|ΠΙΘ)$\")\n",
    "        except14 = re.compile(r\"^(ΔΙΑΘ|Θ|ΠΑΡΑΚΑΤΑΘ|ΠΡΟΣΘ|ΣΥΝΘ|)$\")\n",
    "        if (except13.match(w)) or (except14.match(w)):\n",
    "            w = w + \"ΗΚ\"\n",
    "\n",
    "    # Step 5h\n",
    "    pattern = re.compile(r\"^(.+?)(ΟΥΣΑ|ΟΥΣΕΣ|ΟΥΣΕ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except15 = re.compile(\n",
    "            r\"^(ΦΑΡΜΑΚ|ΧΑΔ|ΑΓΚ|ΑΝΑΡΡ|ΒΡΟΜ|ΕΚΛΙΠ|ΛΑΜΠΙΔ|ΛΕΧ|Μ|ΠΑΤ|Ρ|Λ|ΜΕΔ|ΜΕΣΑΖ|ΥΠΟΤΕΙΝ|ΑΜ|ΑΙΘ|ΑΝΗΚ|ΔΕΣΠΟΖ|ΕΝΔΙΑΦΕΡ|ΔΕ|ΔΕΥΤΕΡΕΥ|ΚΑΘΑΡΕΥ|ΠΛΕ|ΤΣΑ)$\")\n",
    "        except16 = re.compile(r\"(ΠΟΔΑΡ|ΒΛΕΠ|ΠΑΝΤΑΧ|ΦΡΥΔ|ΜΑΝΤΙΛ|ΜΑΛΛ|ΚΥΜΑΤ|ΛΑΧ|ΛΗΓ|ΦΑΓ|ΟΜ|ΠΡΩΤ)$\")\n",
    "        if (except15.match(w)) or (except16.match(w)):\n",
    "            w = w + \"ΟΥΣ\"\n",
    "\n",
    "    # Step 5i\n",
    "    pattern = re.compile(r\"^(.+?)(ΑΓΑ|ΑΓΕΣ|ΑΓΕ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except17 = re.compile(r\"^(ΨΟΦ|ΝΑΥΛΟΧ)$\")\n",
    "        except20 = re.compile(r\"(ΚΟΛΛ)$\")\n",
    "        except18 = re.compile(\n",
    "            r\"^(ΑΒΑΣΤ|ΠΟΛΥΦ|ΑΔΗΦ|ΠΑΜΦ|Ρ|ΑΣΠ|ΑΦ|ΑΜΑΛ|ΑΜΑΛΛΙ|ΑΝΥΣΤ|ΑΠΕΡ|ΑΣΠΑΡ|ΑΧΑΡ|ΔΕΡΒΕΝ|ΔΡΟΣΟΠ|ΞΕΦ|ΝΕΟΠ|ΝΟΜΟΤ|ΟΛΟΠ|ΟΜΟΤ|ΠΡΟΣΤ|ΠΡΟΣΩΠΟΠ|ΣΥΜΠ|ΣΥΝΤ|Τ|ΥΠΟΤ|ΧΑΡ|ΑΕΙΠ|ΑΙΜΟΣΤ|ΑΝΥΠ|ΑΠΟΤ|ΑΡΤΙΠ|ΔΙΑΤ|ΕΝ|ΕΠΙΤ|ΚΡΟΚΑΛΟΠ|ΣΙΔΗΡΟΠ|Λ|ΝΑΥ|ΟΥΛΑΜ|ΟΥΡ|Π|ΤΡ|Μ)$\")\n",
    "        except19 = re.compile(r\"(ΟΦ|ΠΕΛ|ΧΟΡΤ|ΛΛ|ΣΦ|ΡΠ|ΦΡ|ΠΡ|ΛΟΧ|ΣΜΗΝ)$\")\n",
    "        if (except18.match(w) and except19.match(w)) and not ((except17.match(w)) or (except20.match(w))):\n",
    "            w = w + \"ΑΓ\"\n",
    "\n",
    "    # Step 5j\n",
    "    pattern = re.compile(\"^(.+?)(ΗΣΕ|ΗΣΟΥ|ΗΣΑ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except21 = re.compile(r\"^(Ν|ΧΕΡΣΟΝ|ΔΩΔΕΚΑΝ|ΕΡΗΜΟΝ|ΜΕΓΑΛΟΝ|ΕΠΤΑΝ)$\")\n",
    "        if except21.match(w):\n",
    "            w = w + \"ΗΣ\"\n",
    "\n",
    "    # Step 5k\n",
    "    pattern = re.compile(r\"^(.+?)(ΗΣΤΕ)$\")\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except22 = re.compile(r\"^(ΑΣΒ|ΣΒ|ΑΧΡ|ΧΡ|ΑΠΛ|ΑΕΙΜΝ|ΔΥΣΧΡ|ΕΥΧΡ|ΚΟΙΝΟΧΡ|ΠΑΛΙΜΨ)$\")\n",
    "        if except22.match(w):\n",
    "            w = w + \"ΗΣΤ\"\n",
    "\n",
    "    # Step 5l\n",
    "    pattern = re.compile(\"^(.+?)(ΟΥΝΕ|ΗΣΟΥΝΕ|ΗΘΟΥΝΕ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except23 = re.compile(\"^(Ν|Ρ|ΣΠΙ|ΣΤΡΑΒΟΜΟΥΤΣ|ΚΑΚΟΜΟΥΤΣ|ΕΞΩΝ)$\")\n",
    "        if except23.match(w):\n",
    "            w = w + \"ΟΥΝ\"\n",
    "\n",
    "    # Step 5l\n",
    "    pattern = re.compile(r\"^(.+?)(ΟΥΜΕ|ΗΣΟΥΜΕ|ΗΘΟΥΜΕ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "        test1 = False\n",
    "        except24 = re.compile(r\"^(ΠΑΡΑΣΟΥΣ|Φ|Χ|ΩΡΙΟΠΛ|ΑΖ|ΑΛΛΟΣΟΥΣ|ΑΣΟΥΣ)$\")\n",
    "        if except24.match(w):\n",
    "            w = w + \"ΟΥΜ\"\n",
    "\n",
    "    # Step 6\n",
    "    pattern = re.compile(r\"^(.+?)(ΜΑΤΑ|ΜΑΤΩΝ|ΜΑΤΟΣ)$\")\n",
    "    pattern2 = re.compile(\n",
    "        r\"^(.+?)(Α|ΑΓΑΤΕ|ΑΓΑΝ|ΑΕΙ|ΑΜΑΙ|ΑΝ|ΑΣ|ΑΣΑΙ|ΑΤΑΙ|ΑΩ|Ε|ΕΙ|ΕΙΣ|ΕΙΤΕ|ΕΣΑΙ|ΕΣ|ΕΤΑΙ|Ι|ΙΕΜΑΙ|ΙΕΜΑΣΤΕ|ΙΕΤΑΙ|ΙΕΣΑΙ|ΙΕΣΑΣΤΕ|ΙΟΜΑΣΤΑΝ|ΙΟΜΟΥΝ|ΙΟΜΟΥΝΑ|ΙΟΝΤΑΝ|ΙΟΝΤΟΥΣΑΝ|ΙΟΣΑΣΤΑΝ|ΙΟΣΑΣΤΕ|ΙΟΣΟΥΝ|ΙΟΣΟΥΝΑ|ΙΟΤΑΝ|ΙΟΥΜΑ|ΙΟΥΜΑΣΤΕ|ΙΟΥΝΤΑΙ|ΙΟΥΝΤΑΝ|Η|ΗΔΕΣ|ΗΔΩΝ|ΗΘΕΙ|ΗΘΕΙΣ|ΗΘΕΙΤΕ|ΗΘΗΚΑΤΕ|ΗΘΗΚΑΝ|ΗΘΟΥΝ|ΗΘΩ|ΗΚΑΤΕ|ΗΚΑΝ|ΗΣ|ΗΣΑΝ|ΗΣΑΤΕ|ΗΣΕΙ|ΗΣΕΣ|ΗΣΟΥΝ|ΗΣΩ|Ο|ΟΙ|ΟΜΑΙ|ΟΜΑΣΤΑΝ|ΟΜΟΥΝ|ΟΜΟΥΝΑ|ΟΝΤΑΙ|ΟΝΤΑΝ|ΟΝΤΟΥΣΑΝ|ΟΣ|ΟΣΑΣΤΑΝ|ΟΣΑΣΤΕ|ΟΣΟΥΝ|ΟΣΟΥΝΑ|ΟΤΑΝ|ΟΥ|ΟΥΜΑΙ|ΟΥΜΑΣΤΕ|ΟΥΝ|ΟΥΝΤΑΙ|ΟΥΝΤΑΝ|ΟΥΣ|ΟΥΣΑΝ|ΟΥΣΑΤΕ|Υ|ΥΣ|Ω|ΩΝ)$\")\n",
    "\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem + \"ΜΑ\"\n",
    "\n",
    "    if pattern2.match(w) and test1:\n",
    "        fp = pattern2.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "\n",
    "    # Step 7 (ΠΑΡΑΘΕΤΙΚΑ)\n",
    "    pattern = re.compile(r\"^(.+?)(ΕΣΤΕΡ|ΕΣΤΑΤ|ΟΤΕΡ|ΟΤΑΤ|ΥΤΕΡ|ΥΤΑΤ|ΩΤΕΡ|ΩΤΑΤ)$\")\n",
    "    if pattern.match(w):\n",
    "        fp = pattern.match(w).groups()\n",
    "        stem = fp[0]\n",
    "        w = stem\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db223089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.271805Z",
     "iopub.status.busy": "2023-12-13T19:02:38.271454Z",
     "iopub.status.idle": "2023-12-13T19:02:38.292368Z",
     "shell.execute_reply": "2023-12-13T19:02:38.291402Z"
    },
    "papermill": {
     "duration": 0.030871,
     "end_time": "2023-12-13T19:02:38.294632",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.263761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sanitize:\n",
    "    HYPERLINKS_REGEX = r'(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|(www\\.[^ \\s]+)'\n",
    "    HASHTAGS_REGEX = r'#\\w+'\n",
    "    MENTIONS_REGEX = r'@\\w+'\n",
    "\n",
    "    _stopwordsPath = \"Data/Greek-Stopwords.txt\"\n",
    "    greekStopwords = Stopwords(_stopwordsPath)\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean(text, breakSpecials):\n",
    "        \"\"\"\n",
    "        Utility method that tokenizes a text with the option to either break special characters like \"!!!\" into \"!\", \"!\", \"!\" or keep them together.\n",
    "        \"\"\"\n",
    "        def isAcceptable(character):\n",
    "            return character.isalnum() or character == \" \"\n",
    "\n",
    "        cleanedText = \"\"\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            character = text[i]\n",
    "            if character.isalnum() or character == \" \":\n",
    "                cleanedText += character\n",
    "                i += 1\n",
    "            else:\n",
    "                if not breakSpecials:\n",
    "                    newToken = \"\"\n",
    "                    j = i\n",
    "                    while j < len(text) and not isAcceptable(text[j]):\n",
    "                        newToken += text[j]\n",
    "                        j += 1\n",
    "                        if j < len(text) and text[j] != text[j - 1]:\n",
    "                            break\n",
    "\n",
    "                    cleanedText += f\" {newToken} \"\n",
    "                    i = j\n",
    "                else:\n",
    "                    cleanedText += f\" {text[i]} \"\n",
    "                    i += 1\n",
    "\n",
    "        return \" \".join(cleanedText.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def customTokenize(text, breakSpecials):\n",
    "        return Sanitize._clean(text, breakSpecials).split(), []\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def removeAccents(token):\n",
    "        \"\"\"\n",
    "        Utility method that eliminates a token's accents.\n",
    "        \"\"\"\n",
    "        return ''.join(\n",
    "            character for character in unicodedata.normalize('NFD', token) if unicodedata.category(character) != 'Mn')\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def removeSpecialCharacters(token):\n",
    "        \"\"\"\n",
    "        Utility method that eliminates a token's special characters.\n",
    "        \"\"\"\n",
    "        refinedToken = \"\"\n",
    "        for character in token:\n",
    "            if character.isalnum():\n",
    "                refinedToken += character\n",
    "        return refinedToken\n",
    "\n",
    "    @staticmethod\n",
    "    def isNumeric(token):\n",
    "        \"\"\"\n",
    "        Utility method that checks if a token is numeric. \n",
    "        A token is considered numeric if it consists only of numbers and its length is not equal to 4, accounting for cases like dates.\n",
    "        \"\"\"\n",
    "        for character in token:\n",
    "            if not character.isdigit():\n",
    "                return False\n",
    "\n",
    "        return True if len(token) != 4 else False\n",
    "\n",
    "    @staticmethod\n",
    "    def refineHashtag(hashtag):\n",
    "        \"\"\"\n",
    "        Utility method to refine a hashtag by: \n",
    "        Capitalizing it.\n",
    "        Eliminating special characters, as hashtags like #EKLOGES2019 and #EKLOGES_2019 signify the same topic.\n",
    "        Removing accents.\n",
    "        \"\"\"\n",
    "        refinedHashtag = hashtag.upper()\n",
    "        refinedHashtag = Sanitize.removeAccents(refinedHashtag)\n",
    "        refinedHashtag = Sanitize.removeSpecialCharacters(refinedHashtag)\n",
    "        return f\"#{refinedHashtag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdd700b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.309270Z",
     "iopub.status.busy": "2023-12-13T19:02:38.308595Z",
     "iopub.status.idle": "2023-12-13T19:02:38.321440Z",
     "shell.execute_reply": "2023-12-13T19:02:38.320357Z"
    },
    "papermill": {
     "duration": 0.022729,
     "end_time": "2023-12-13T19:02:38.323776",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.301047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, inputSize, seed=None):\n",
    "        \"\"\"\n",
    "        Utility class that implements a neural network following a TensorFlow-like structure.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        if seed is not None:\n",
    "            Model.enableDeterminism(seed)\n",
    "        self.__inputSize = inputSize\n",
    "        self.__created = False\n",
    "        self.__model = None\n",
    "        self.__layers = []\n",
    "        self.__previousLayerOutput = -1\n",
    "        self.__shouldEvaluate = False\n",
    "\n",
    "    def build(self):\n",
    "        self.__created = True\n",
    "        self.__model = nn.Sequential(*self.__layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.__created:\n",
    "            return self.__model(x)\n",
    "        return None\n",
    "\n",
    "    def addDropout(self, p):\n",
    "        if not self.__created:\n",
    "            self.__layers.append(nn.Dropout(p))\n",
    "            self.__shouldEvaluate = True\n",
    "\n",
    "    def addDense(self, neurons):\n",
    "        if not self.__created:\n",
    "            if self.__previousLayerOutput == -1:\n",
    "                self.__layers.append(nn.Linear(self.__inputSize, neurons))\n",
    "            else:\n",
    "                self.__layers.append(nn.Linear(self.__previousLayerOutput, neurons))\n",
    "            self.__previousLayerOutput = neurons\n",
    "\n",
    "    def addRelu(self):\n",
    "        if not self.__created:\n",
    "            self.__layers.append(nn.ReLU())\n",
    "\n",
    "    def addLeakyRelu(self, alpha=1e-2):\n",
    "        if not self.__created:\n",
    "            self.__layers.append(nn.LeakyReLU(negative_slope=alpha))\n",
    "\n",
    "    def addSoftplus(self, beta=1, threshold=20):\n",
    "        if not self.__created:\n",
    "            self.__layers.append(nn.Softplus(beta=beta, threshold=threshold))\n",
    "\n",
    "    def addSigmoid(self):\n",
    "        if not self.__created:\n",
    "            self.__layers.append(nn.Sigmoid())\n",
    "\n",
    "    def addBatchNormalization(self):\n",
    "        if not self.__created:\n",
    "            if self.__previousLayerOutput == -1:\n",
    "                self.__layers.append(nn.BatchNorm1d(self.__inputSize))\n",
    "            else:\n",
    "                self.__layers.append(nn.BatchNorm1d(self.__previousLayerOutput))\n",
    "\n",
    "    def shouldEvaluate(self):\n",
    "        return self.__shouldEvaluate\n",
    "\n",
    "    @staticmethod\n",
    "    def saveModel(stateDict, path):\n",
    "        torch.save(stateDict, f\"{path}\\\\Model.pth\")\n",
    "\n",
    "    @staticmethod\n",
    "    def enableDeterminism(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        numpy.random.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2b36ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.338172Z",
     "iopub.status.busy": "2023-12-13T19:02:38.337777Z",
     "iopub.status.idle": "2023-12-13T19:02:38.351257Z",
     "shell.execute_reply": "2023-12-13T19:02:38.350196Z"
    },
    "papermill": {
     "duration": 0.023286,
     "end_time": "2023-12-13T19:02:38.353373",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.330087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "\n",
    "    def __init__(self, model, metricName, patience, delta, maximize):\n",
    "        \"\"\"\n",
    "        A utility class implementing the early stopping concept with parameters for patience and delta.\n",
    "        This class stores a snapshot of the training session by saving the best epoch, the optimal value for the chosen metric\n",
    "        and the labels and predicted labels for both datasets, as the model will be evaluated based on its best epoch.\n",
    "        \"\"\"\n",
    "        self.__metricName = metricName.upper()\n",
    "        self.__maximize = maximize\n",
    "        self.__patience = patience\n",
    "        self.__delta = delta\n",
    "        self.__counter = 0\n",
    "        self.__bestValue = float('-inf') if maximize else float('inf')\n",
    "        self.__bestEpoch = 0\n",
    "        self.__state = copy.deepcopy(model.state_dict())\n",
    "        self.__shouldStop = False\n",
    "        self.__trainingLabels = None\n",
    "        self.__predictedTrainingLabels = None\n",
    "        self.__validationLabels = None\n",
    "        self.__predictedValidationLabels = None\n",
    "\n",
    "    def update(self, value, epoch, model, trainingLabels, predictedTrainingLabels, validationLabels, predictedValidationLabels):\n",
    "        if not self.__maximize:\n",
    "            # If the current value is lower than the best value minus the delta parameter, store all the required information and reset the counter.\n",
    "            if value < self.__bestValue - self.__delta:\n",
    "                self.__bestValue = value\n",
    "                self.__bestEpoch = epoch\n",
    "                self.__state = copy.deepcopy(model.state_dict())\n",
    "                self.__trainingLabels = copy.deepcopy(trainingLabels)\n",
    "                self.__predictedTrainingLabels = copy.deepcopy(predictedTrainingLabels)\n",
    "                self.__validationLabels = copy.deepcopy(validationLabels)\n",
    "                self.__predictedValidationLabels = copy.deepcopy(predictedValidationLabels)\n",
    "                self.__counter = 0\n",
    "            # Otherwise, increment the counter and verify if we have exceeded the patience limit.\n",
    "            else:\n",
    "                self.__counter += 1\n",
    "                if self.__counter >= self.__patience:\n",
    "                    self.__shouldStop = True\n",
    "\n",
    "        else:\n",
    "            # If the current value is greater than the best value plus the delta parameter, store all the required information and reset the counter.\n",
    "            if value > self.__bestValue + self.__delta:\n",
    "                self.__bestValue = value\n",
    "                self.__bestEpoch = epoch\n",
    "                self.__state = copy.deepcopy(model.state_dict())\n",
    "                self.__trainingLabels = copy.deepcopy(trainingLabels)\n",
    "                self.__predictedTrainingLabels = copy.deepcopy(predictedTrainingLabels)\n",
    "                self.__validationLabels = copy.deepcopy(validationLabels)\n",
    "                self.__predictedValidationLabels = copy.deepcopy(predictedValidationLabels)\n",
    "                self.__counter = 0\n",
    "            # Otherwise, increment the counter and verify if we have exceeded the patience limit.\n",
    "            else:\n",
    "                self.__counter += 1\n",
    "                if self.__counter >= self.__patience:\n",
    "                    self.__shouldStop = True\n",
    "\n",
    "    def shouldStop(self):\n",
    "        return self.__shouldStop\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"EarlyStopper:\\n\"\n",
    "                f\"Metric: {self.__metricName}\\n\"\n",
    "                f\"Maximize: {self.__maximize}\\n\"\n",
    "                f\"Value: {self.__bestValue}\\n\"\n",
    "                f\"Epoch: {self.__bestEpoch}\\n\"\n",
    "                f\"Patience: {self.__patience}\\n\"\n",
    "                f\"Delta: {self.__delta}\")\n",
    "\n",
    "    def getMetric(self):\n",
    "        return self.__metricName\n",
    "\n",
    "    def getEpoch(self):\n",
    "        return self.__bestEpoch\n",
    "\n",
    "    def getState(self):\n",
    "        return self.__state\n",
    "\n",
    "    def getTrainingLabels(self):\n",
    "        return self.__trainingLabels, self.__predictedTrainingLabels\n",
    "\n",
    "    def getValidationLabels(self):\n",
    "        return self.__validationLabels, self.__predictedValidationLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0396de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.367671Z",
     "iopub.status.busy": "2023-12-13T19:02:38.367333Z",
     "iopub.status.idle": "2023-12-13T19:02:38.373429Z",
     "shell.execute_reply": "2023-12-13T19:02:38.372330Z"
    },
    "papermill": {
     "duration": 0.015803,
     "end_time": "2023-12-13T19:02:38.375429",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.359626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomScheduler:\n",
    "    def __init__(self, scheduler, epoch=None):\n",
    "        \"\"\"\n",
    "        Utility class implementing a custom scheduler that triggers the decay of the learning rate only after a specified number of epochs.\n",
    "        \"\"\"\n",
    "        self.__scheduler = scheduler\n",
    "        self.__epoch = epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if self.__epoch is not None and epoch >= self.__epoch:\n",
    "            self.__scheduler.step()\n",
    "        else:\n",
    "            self.__scheduler.step()\n",
    "            \n",
    "    def __str__(self):\n",
    "        return (f\"Custom Scheduler\\n\"\n",
    "                f\"Epoch: {self.__epoch}\\n\"\n",
    "                f\"Scheduler:\\n\"\n",
    "                f\"gamma: {self.__scheduler.gamma}\\n\"\n",
    "                f\"step: {self.__scheduler.step_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba1107d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.389604Z",
     "iopub.status.busy": "2023-12-13T19:02:38.389266Z",
     "iopub.status.idle": "2023-12-13T19:02:38.403842Z",
     "shell.execute_reply": "2023-12-13T19:02:38.402873Z"
    },
    "papermill": {
     "duration": 0.024187,
     "end_time": "2023-12-13T19:02:38.405915",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.381728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitizeDataset(path, isTest):\n",
    "    \"\"\"\n",
    "    A utility function designed to preprocess the given dataset.\n",
    "    \"\"\"\n",
    "    dataframe = pandas.read_csv(path, encoding=\"utf-8\")\n",
    "    tweets = []\n",
    "    \n",
    "    hashtagPlaceholder = \"TWEETHASHTAGPLACEHOLDER\"\n",
    "    linkPlaceholder = \"TWEETLINKPLACEHOLDER\"\n",
    "    mentionPlaceholder = \"TWEETMENTIONPLACEHOLDER\"\n",
    "    designatedMentions = {\"@ATSIPRAS\", \"@YANISVAROUFAKIS\", \"@KMITSOTAKIS\", \"@VELOPKY\", \"@ELLINIKILISI\",\n",
    "                          \"@ADONISGEORGIADI\", \"@SKAIGR\", \"@NEADEMOKRATIA\", \"@AVGIONLINE\", \"@ΝΕΑ\"}\n",
    "\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "\n",
    "        tweet = Tweet()\n",
    "\n",
    "        tweet.setID(row['New_ID'])\n",
    "        tweet.setParty(row['Party'])\n",
    "\n",
    "        if not isTest:\n",
    "            tweet.setSentiment(Tweet.classToNumber[row['Sentiment']])\n",
    "\n",
    "    \n",
    "        text = row['Text']\n",
    "        \n",
    "\n",
    "        # Identify and extract all links from the tweet.\n",
    "        # Store all the links from the tweet.\n",
    "        # Substitute all the identified links in the tweet with a placeholder for links. \n",
    "        # This approach was implemented to retain the links' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        links = re.findall(Sanitize.HYPERLINKS_REGEX, text)\n",
    "        links = [\"\".join(group) for group in links]\n",
    "        tweet.addLinks(links)\n",
    "        replacement = f\" {linkPlaceholder} \"\n",
    "        for link in links:\n",
    "            text = text.replace(link, replacement)\n",
    "        \n",
    "        # Identify and extract all mentions from the tweet.\n",
    "        # Store all the mentions from the tweet.\n",
    "        # Substitute all the identified mentions in the tweet with a placeholder for mentions. \n",
    "        # This approach was implemented to retain the mentions' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        mentions = re.findall(Sanitize.MENTIONS_REGEX, text)\n",
    "        tweet.addMentions(mentions)\n",
    "        replacement = f\" {mentionPlaceholder} \"\n",
    "        for mention in mentions:\n",
    "            text = text.replace(mention, replacement)\n",
    "            \n",
    "        # Identify and extract all hashtags from the tweet.\n",
    "        # Store all the hashtags from the tweet.\n",
    "        # Substitute all the identified hashtags in the tweet with a placeholder for hashtags. \n",
    "        # This approach was implemented to retain the hashtags' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        hashtags = re.findall(Sanitize.HASHTAGS_REGEX, text)\n",
    "        tweet.addHashtags(hashtags)\n",
    "        replacement = f\" {hashtagPlaceholder} \"\n",
    "        for hashtag in hashtags:\n",
    "            text = text.replace(hashtag, replacement)\n",
    "        \n",
    "        # Transform the text of the tweet to uppercase and replace Ν.Δ. with ΝΔ.\n",
    "        text = text.upper()\n",
    "        text = text.replace(\"Ν.Δ.\", \" ΝΔ \")\n",
    "\n",
    "        tweetTokens, lemmatizedTokens = Sanitize.customTokenize(text, breakSpecials=True)\n",
    "        refinedTokens = []\n",
    "        hashtagIndex = 0\n",
    "        mentionIndex = 0\n",
    "        \n",
    "        for token in tweetTokens:\n",
    "            \n",
    "            # If the corresponding token is a hashtag, refine and append it to the list of tokens.\n",
    "            if token == hashtagPlaceholder:\n",
    "                refinedToken = hashtags[hashtagIndex]\n",
    "                refinedToken = Sanitize.refineHashtag(refinedToken)\n",
    "                refinedTokens.append(refinedToken)\n",
    "                hashtagIndex += 1\n",
    "                continue\n",
    "            \n",
    "            # If the corresponding token is a link ignore it.\n",
    "            if token == linkPlaceholder:\n",
    "                continue\n",
    "            \n",
    "            # If the corresponding token is a mention, convert it to uppercase and \n",
    "            # include it in the list of tokens only if it belongs to the specified set of designated mentions.\n",
    "            if token == mentionPlaceholder:\n",
    "                refinedToken = mentions[mentionIndex].upper()\n",
    "                mentionIndex += 1\n",
    "                if refinedToken in designatedMentions:\n",
    "                    refinedTokens.append(refinedToken)\n",
    "                continue\n",
    "            \n",
    "            # Eliminate special characters.\n",
    "            if len(Sanitize.removeSpecialCharacters(token)) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Eliminate numeric characters.\n",
    "            if Sanitize.isNumeric(token):\n",
    "                continue\n",
    "\n",
    "            # Eliminate accents.\n",
    "            refinedToken = Sanitize.removeAccents(token)\n",
    "            \n",
    "            # If the corresponding token is a stopword ignore it.\n",
    "            if refinedToken in Sanitize.greekStopwords:\n",
    "                continue\n",
    "            \n",
    "            # Stem the corresponding token.\n",
    "            refinedToken = stemWord(refinedToken)\n",
    "\n",
    "            if len(refinedToken) > 0:\n",
    "                refinedTokens.append(refinedToken)\n",
    "\n",
    "        tweet.setText(\" \".join(refinedTokens))\n",
    "\n",
    "        tweets.append(tweet)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7957ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:38.420136Z",
     "iopub.status.busy": "2023-12-13T19:02:38.419753Z",
     "iopub.status.idle": "2023-12-13T19:02:39.278895Z",
     "shell.execute_reply": "2023-12-13T19:02:39.277916Z"
    },
    "papermill": {
     "duration": 0.869175,
     "end_time": "2023-12-13T19:02:39.281369",
     "exception": false,
     "start_time": "2023-12-13T19:02:38.412194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    embeddingsPath = \"Data/Vectors.txt\"\n",
    "    embeddings = Embeddings(embeddingsPath)\n",
    "    encounteredParties = dict()\n",
    "    partyIndex = 1.0\n",
    "    scaler = None\n",
    "    dimensions = embeddings.dimensions() + 2 \n",
    "\n",
    "    def __init__(self, tweets, isTraining, aggregator, scaler=None):\n",
    "        self.__instances = None\n",
    "        self.__labels = None\n",
    "        self.__ids = None\n",
    "        if isTraining:\n",
    "            TweetDataset.scaler = scaler\n",
    "        \n",
    "        self.__transform(tweets, isTraining, aggregator)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__instances[index], self.__labels[index]\n",
    "    \n",
    "    def getInstances(self):\n",
    "        return self.__instances\n",
    "    \n",
    "    def getIDS(self):\n",
    "        return self.__ids\n",
    "    \n",
    "    def __transform(self, tweets, isTraining, aggregator):\n",
    "        ids = []\n",
    "        labels = []\n",
    "        instances = []\n",
    "        \n",
    "        # If referring to the training set and the parties have not yet been recorded.\n",
    "        if isTraining and len(TweetDataset.encounteredParties.keys()) == 0:\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                \n",
    "                # Assign a distinct numerical identifier to each party.\n",
    "                if tweet.getParty() not in TweetDataset.encounteredParties.keys():\n",
    "                    TweetDataset.encounteredParties[tweet.getParty()] = TweetDataset.partyIndex\n",
    "                    TweetDataset.partyIndex += 1.0\n",
    "            \n",
    "            # Normalize the value of each party by dividing it by the highest party value.\n",
    "            for key in TweetDataset.encounteredParties.keys():\n",
    "                TweetDataset.encounteredParties[key] /= (TweetDataset.partyIndex - 1.0)\n",
    "\n",
    "        for tweet in tweets:\n",
    "\n",
    "            ids.append(tweet.getID())\n",
    "            labels.append(tweet.getSentiment())\n",
    "\n",
    "            tokens = tweet.getText().split()\n",
    "            \n",
    "            # Aggregate the embeddings/tokens of the respective tweet.\n",
    "            instance = aggregator(tokens)\n",
    "            \n",
    "            # Incorporate the party information of the associated tweet as a feature column.\n",
    "            if tweet.getParty() in TweetDataset.encounteredParties.keys():\n",
    "                party = TweetDataset.encounteredParties[tweet.getParty()]\n",
    "                instance = torch.cat((instance, torch.tensor([party], dtype=torch.float32)))\n",
    "            \n",
    "            # Exclude the party of the tweet if it was not encountered in the training set.\n",
    "            else:\n",
    "                instance = torch.cat((instance, torch.tensor([0.0], dtype=torch.float32)))\n",
    "            \n",
    "            # Incorporate the has or has not links information of the associated tweet as a feature column.\n",
    "            links = 1.0 if len(tweet.getLinks()) > 0 else 0.0\n",
    "            instance = torch.cat((instance, torch.tensor([links], dtype=torch.float32)))\n",
    "\n",
    "            instances.append(instance)\n",
    "\n",
    "        instances = torch.stack(instances)\n",
    "\n",
    "        if TweetDataset.scaler is not None:\n",
    "            # If referring to the training set and the scaler has not yet been fitted.\n",
    "            if isTraining:\n",
    "                TweetDataset.scaler.fit(instances.numpy())\n",
    "            \n",
    "            # Apply scaling to the instances.\n",
    "            instances = TweetDataset.scaler.transform(instances.numpy())\n",
    "\n",
    "        self.__instances = torch.tensor(instances, dtype=torch.float32)\n",
    "        if None not in labels:\n",
    "            self.__labels = torch.LongTensor(labels)\n",
    "        self.__ids = ids\n",
    "\n",
    "    @staticmethod\n",
    "    def average(tokens):\n",
    "        \"\"\"\n",
    "        Utility function to aggregate a tweet's embeddings by averaging the embeddings of its tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If no embeddings are found for a tweet's tokens or if the tweet is empty, a list of zeros will be returned.\n",
    "        instance = torch.zeros((TweetDataset.embeddings.dimensions(),), dtype=torch.float32)\n",
    "\n",
    "        words = 0.0\n",
    "        for token in tokens:\n",
    "            if token in TweetDataset.embeddings:\n",
    "                # Increment the counter for each token in the tweet for which an embedding exists.\n",
    "                words += 1.0\n",
    "                instance = instance + torch.tensor(TweetDataset.embeddings[token], dtype=torch.float32)\n",
    "\n",
    "        if words != 0.0:\n",
    "            instance = instance / words\n",
    "\n",
    "        return instance\n",
    "\n",
    "    @staticmethod\n",
    "    def sum(tokens):\n",
    "        \"\"\"\n",
    "        Utility function to aggregate a tweet's embeddings by summing the embeddings of its tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        instance = torch.zeros((TweetDataset.embeddings.dimensions(),), dtype=torch.float32)\n",
    "\n",
    "        for token in tokens:\n",
    "            instance = instance + torch.tensor(TweetDataset.embeddings[token], dtype=torch.float32)\n",
    "\n",
    "        return instance\n",
    "    \n",
    "    @staticmethod\n",
    "    def max(tokens):\n",
    "        \"\"\"\n",
    "        Utility function to aggregate a tweet's embeddings by choosing the maximum value across each dimension.\n",
    "        \"\"\"\n",
    "        \n",
    "        tweetEmbeddings = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            tweetEmbeddings.append(TweetDataset.embeddings[token])\n",
    "        \n",
    "        # Choose the maximum value across each dimension\n",
    "        if len(tweetEmbeddings) > 0:\n",
    "            tweetEmbeddings = numpy.max(tweetEmbeddings, axis=0)\n",
    "        \n",
    "        # If no embeddings are found for a tweet's tokens or if the tweet is empty, return a list of zeros.\n",
    "        else:\n",
    "            tweetEmbeddings = numpy.zeros((TweetDataset.embeddings.dimensions(),), dtype=numpy.float32)\n",
    "        \n",
    "        return torch.tensor(tweetEmbeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "465dc6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:02:39.295797Z",
     "iopub.status.busy": "2023-12-13T19:02:39.295388Z",
     "iopub.status.idle": "2023-12-13T19:03:14.962814Z",
     "shell.execute_reply": "2023-12-13T19:03:14.961905Z"
    },
    "papermill": {
     "duration": 35.67776,
     "end_time": "2023-12-13T19:03:14.965494",
     "exception": false,
     "start_time": "2023-12-13T19:02:39.287734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingSetPath = \"Data/train_set.csv\"\n",
    "validationSetPath = \"Data/valid_set.csv\"\n",
    "testingSetPath = \"Data/test_set.csv\"\n",
    "\n",
    "trainingTweets = sanitizeDataset(path=trainingSetPath, isTest=False)\n",
    "\n",
    "validationTweets = sanitizeDataset(path=validationSetPath, isTest=False)\n",
    "\n",
    "testingTweets = sanitizeDataset(path=testingSetPath, isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2022e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:03:15.000782Z",
     "iopub.status.busy": "2023-12-13T19:03:14.999872Z",
     "iopub.status.idle": "2023-12-13T19:03:27.768622Z",
     "shell.execute_reply": "2023-12-13T19:03:27.767446Z"
    },
    "papermill": {
     "duration": 12.778825,
     "end_time": "2023-12-13T19:03:27.771428",
     "exception": false,
     "start_time": "2023-12-13T19:03:14.992603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "aggregators = [TweetDataset.sum, TweetDataset.average, TweetDataset.max]\n",
    "scalers = [None, MinMaxScaler(feature_range=(-1, 1)), StandardScaler()]\n",
    "\n",
    "aggregator = aggregators[0]\n",
    "scaler = scalers[1]\n",
    "\n",
    "trainingDataset = TweetDataset(tweets=trainingTweets,\n",
    "                               isTraining=True,\n",
    "                               scaler=scaler,\n",
    "                               aggregator=aggregator)\n",
    "\n",
    "validationDataset = TweetDataset(tweets=validationTweets,\n",
    "                                 isTraining=False,\n",
    "                                 scaler=scaler,\n",
    "                                 aggregator=aggregator)\n",
    "\n",
    "testingDataset = TweetDataset(tweets=testingTweets,\n",
    "                              isTraining=False,\n",
    "                              scaler=scaler,\n",
    "                              aggregator=aggregator)\n",
    "\n",
    "\n",
    "batchSize = 64\n",
    "trainDataloader = DataLoader(trainingDataset, batch_size=batchSize, shuffle=True)\n",
    "validationDataloader = DataLoader(validationDataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f5fa8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:03:27.786841Z",
     "iopub.status.busy": "2023-12-13T19:03:27.786406Z",
     "iopub.status.idle": "2023-12-13T19:03:27.830354Z",
     "shell.execute_reply": "2023-12-13T19:03:27.829120Z"
    },
    "papermill": {
     "duration": 0.054737,
     "end_time": "2023-12-13T19:03:27.832842",
     "exception": false,
     "start_time": "2023-12-13T19:03:27.778105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateModel(predictions, labels, average=\"weighted\"):\n",
    "    \"\"\"\n",
    "    Utility function to evaluate a model's F1, recall, precision and accuracy scores.\n",
    "    \"\"\"\n",
    "    # Convert predictions and labels to NumPy arrays.\n",
    "    predictions = predictions.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    # Calculate F1, recall, precision, and accuracy.\n",
    "    f1 = f1_score(labels, predictions, average=average)\n",
    "    recall = recall_score(labels, predictions, average=average)\n",
    "    precision = precision_score(labels, predictions, average=average)\n",
    "    accuracy = numpy.sum(predictions == labels) / len(labels)\n",
    "\n",
    "    return f1, recall, precision, accuracy\n",
    "\n",
    "\n",
    "def plotLearningCurve(xAxis, toPlot, xAxisName, yAxisName, logger):\n",
    "    \"\"\"\n",
    "    Utility function to plot the learning curve for training and validation metrics over epochs.\n",
    "    \"\"\"\n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot each array in 'toPlot' with a corresponding label.\n",
    "    for array, label in toPlot:\n",
    "        pyplot.plot(xAxis, array, label=label)\n",
    "\n",
    "    # Set axis labels, legend, and display/store the plot.\n",
    "    pyplot.xlabel(xAxisName)\n",
    "    pyplot.ylabel(yAxisName)\n",
    "    pyplot.legend()\n",
    "    pyplot.grid(True)\n",
    "    \n",
    "    # Store the plot if a logger is provided.\n",
    "    if logger is not None:\n",
    "        pyplot.savefig(f\"{logger.directory()}\\\\{yAxisName}.png\")\n",
    "        \n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def plotConfusionMatrix(labels, predictedLabels, logger, title):\n",
    "    \"\"\"\n",
    "    Utility function to plot and store the confusion matrix based on true and predicted labels.\n",
    "    \"\"\"    \n",
    "    # Calculate and display the confusion matrix.\n",
    "    sentimentLabels = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "    confusionMatrix = confusion_matrix(labels, predictedLabels, labels=[0, 1, 2])\n",
    "    confusionMatrixDisplay = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=sentimentLabels)\n",
    "    confusionMatrixDisplay.plot(colorbar=False)\n",
    "    \n",
    "    # Store the plot if a logger is provided.\n",
    "    if logger is not None:\n",
    "        pyplot.savefig(f\"{logger.directory()}\\\\{title}.png\")\n",
    "        \n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def classificationReport(labels, predictedLabels, logger, title):\n",
    "    \"\"\"\n",
    "    Utility function to generate and log a classification report based on true and predicted labels.\n",
    "    \"\"\"\n",
    "    sentimentLabels = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "    \n",
    "    # Generate and log the classification report.\n",
    "    report = classification_report(labels, predictedLabels, target_names=sentimentLabels, labels=[0, 1, 2])\n",
    "    \n",
    "    # Store the classification report if a logger is provided.\n",
    "    if logger is not None:\n",
    "        logger.log(report, title=title)\n",
    "        \n",
    "    print(title)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, scheduler, epochs, earlyStop, logger):\n",
    "    \n",
    "\n",
    "    trainingLoss = []\n",
    "    validationLoss = []\n",
    "    trainingMetrics = []\n",
    "    validationMetrics = []\n",
    "\n",
    "    trainingLabels = None\n",
    "    predictedTrainingLabels = None\n",
    "    validationLabels = None\n",
    "    predictedValidationLabels = None\n",
    "    \n",
    "    if logger is not None:\n",
    "        logger.log(f\"aggregator: {aggregator.__name__}\")\n",
    "        logger.log(f\"epochs: {epochs}\")\n",
    "        logger.log(f\"batchSize: {batchSize}\")\n",
    "        logger.log(f\"model:\\n{model}\\n\")\n",
    "        logger.log(f\"optimizer:\\n{str(optimizer)}\\n\")\n",
    "        logger.log(f\"scaler:\\n{str(scaler)}\\n\")\n",
    "        logger.log(f\"scheduler:\\n{str(scheduler)}\\n\\n\")\n",
    "    \n",
    "    print(f\"aggregator: {aggregator.__name__}\")\n",
    "    print(f\"epochs: {epochs}\")\n",
    "    print(f\"batchSize: {batchSize}\")\n",
    "    print(f\"model:\\n{model}\\n\")\n",
    "    print(f\"optimizer:\\n{str(optimizer)}\\n\")\n",
    "    print(f\"scaler:\\n{str(scaler)}\\n\")\n",
    "    print(f\"scheduler:\\n{str(scheduler)}\\n\\n\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Switch the model to training mode.\n",
    "        model.train()\n",
    "        epochLoss = 0.0\n",
    "        \n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle \n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        trainingLabels = torch.LongTensor([])\n",
    "        predictedTrainingLabels = torch.LongTensor([])\n",
    "\n",
    "        for instances, labels in trainDataloader:\n",
    "            \n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            trainingLabels = torch.cat((trainingLabels, labels))\n",
    "            \n",
    "            # Forward pass.\n",
    "            logits = model(instances)\n",
    "            probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "            predictedTrainingLabels = torch.cat((predictedTrainingLabels, predictedLabels))\n",
    "            \n",
    "            # Loss calculation.\n",
    "            loss = criterion(logits, labels)\n",
    "            epochLoss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss. \n",
    "        epochLoss /= len(trainDataloader)\n",
    "        trainingLoss.append(epochLoss)\n",
    "        \n",
    "        # Assess and store the training metrics.\n",
    "        metrics = evaluateModel(predictions=predictedTrainingLabels, labels=trainingLabels)\n",
    "        trainingMetrics.append(metrics)\n",
    "    \n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "        epochLoss = 0.0\n",
    "        \n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle \n",
    "        # argument. These labels will be utilized later to assess the model's performance on the validation set.\n",
    "        validationLabels = torch.LongTensor([])\n",
    "        predictedValidationLabels = torch.LongTensor([])\n",
    "        \n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels in validationDataloader:\n",
    "                validationLabels = torch.cat((validationLabels, labels))\n",
    "                \n",
    "                # Forward pass.\n",
    "                logits = model(instances)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedValidationLabels = torch.cat((predictedValidationLabels, predictedLabels))\n",
    "                \n",
    "                # Loss calculation.\n",
    "                epochLoss += criterion(logits, labels).item()\n",
    "        \n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the validation set, as each batch contributes its own individual loss.  \n",
    "        epochLoss /= len(validationDataloader)\n",
    "        validationLoss.append(epochLoss)\n",
    "        \n",
    "        # Evaluate and store validation metrics\n",
    "        metrics = evaluateModel(predictions=predictedValidationLabels, labels=validationLabels)\n",
    "        validationMetrics.append(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch)\n",
    "        \n",
    "        if logger is not None:\n",
    "            logger.log(f'Epoch {epoch + 1}/{epochs}\\n'\n",
    "                       f'Training Loss: {trainingLoss[-1]:.6f}, Validation Loss: {validationLoss[-1]:.6f}\\n'\n",
    "                       f'Training F1: {trainingMetrics[-1][0]:.6f}, Validation F1: {validationMetrics[-1][0]:.6f}\\n')\n",
    "            logger.line()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}\\n'\n",
    "              f'Training Loss: {trainingLoss[-1]:.6f}, Validation Loss: {validationLoss[-1]:.6f}\\n'\n",
    "              f'Training F1: {trainingMetrics[-1][0]:.6f}, Validation F1: {validationMetrics[-1][0]:.6f}\\n'\n",
    "              f'Training Recall: {trainingMetrics[-1][1]:.6f}, Validation Recall: {validationMetrics[-1][1]:.6f}\\n'\n",
    "              f'Training Precision: {trainingMetrics[-1][2]:.6f}, Validation Precision: {validationMetrics[-1][2]:.6f}\\n'\n",
    "              f'Training Accuracy: {trainingMetrics[-1][3]:.6f}, Validation Accuracy: {validationMetrics[-1][3]:.6f}\\n')\n",
    "\n",
    "        if earlyStop is not None:\n",
    "            if earlyStop.getMetric() == \"F1\":\n",
    "                metric = validationMetrics[-1][0]\n",
    "            elif earlyStop.getMetric() == \"RECALL\":\n",
    "                metric = validationMetrics[-1][1]\n",
    "            elif earlyStop.getMetric() == \"PRECISION\":\n",
    "                metric = validationMetrics[-1][2]\n",
    "            elif earlyStop.getMetric() == \"ACCURACY\":\n",
    "                metric = validationMetrics[-1][2]\n",
    "            else:\n",
    "                metric = validationLoss[-1]\n",
    "            \n",
    "            # Update the early stopper.\n",
    "            earlyStop.update(metric, epoch + 1, model, trainingLabels, predictedTrainingLabels, validationLabels, predictedValidationLabels)\n",
    "\n",
    "            if earlyStop.shouldStop():\n",
    "                print(f\"earlyStop:\\n{earlyStop}\")\n",
    "                if logger is not None:\n",
    "                    logger.log(f\"earlyStop:\\n{earlyStop}\")\n",
    "                break\n",
    "    \n",
    "    # If an evaluation on the training set is intended after the training session, such as for a Dropout layer.\n",
    "    if model.shouldEvaluate():\n",
    "        \n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        evaluationLoss = 0.0\n",
    "        \n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle \n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        evaluationLabels = torch.LongTensor([])\n",
    "        predictedEvaluationLabels = torch.LongTensor([])\n",
    "        \n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels in trainDataloader:\n",
    "                evaluationLabels = torch.cat((evaluationLabels, labels))\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = model(instances)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedEvaluationLabels = torch.cat((predictedEvaluationLabels, predictedLabels))\n",
    "\n",
    "                # Loss calculation\n",
    "                evaluationLoss += criterion(logits, labels).item()\n",
    "        \n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss.\n",
    "        evaluationLoss /= len(trainDataloader)\n",
    "        \n",
    "        # Assess and store the validation metrics.\n",
    "        evaluationMetrics = evaluateModel(predictions=predictedEvaluationLabels, labels=evaluationLabels)\n",
    "    \n",
    "    \n",
    "    # If early stopping is applied\n",
    "    if earlyStop is not None:\n",
    "        if logger is not None:\n",
    "            Model.saveModel(earlyStop.getState(), logger.directory())\n",
    "            \n",
    "        # The optimal epoch is the one determined by the early stopper.\n",
    "        epochs = earlyStop.getEpoch()\n",
    "        \n",
    "        # Retrieve the labels and predicted labels for both datasets at the optimal epoch determined by the early stopper.\n",
    "        trainingLabels, predictedTrainingLabels = earlyStop.getTrainingLabels()\n",
    "        validationLabels, predictedValidationLabels = earlyStop.getValidationLabels()\n",
    "        \n",
    "    else:\n",
    "        if logger is not None:\n",
    "            Model.saveModel(model.state_dict(), logger.directory())\n",
    "\n",
    "    epochsAxis = [i + 1 for i in range(epochs)]\n",
    "\n",
    "    \n",
    "    # Obtain the metrics until reaching the optimal epoch, whether it is determined by the early stopper or the final regular training epoch.\n",
    "    trainingLoss = trainingLoss[:epochs]\n",
    "    validationLoss = validationLoss[:epochs]\n",
    "    trainingMetrics = trainingMetrics[:epochs]\n",
    "    validationMetrics = validationMetrics[:epochs]\n",
    "\n",
    "    plotLearningCurve(epochsAxis, [(validationLoss, 'Validation'), (trainingLoss, 'Training')], 'Epoch', 'Loss', logger)\n",
    "\n",
    "    METRICS = [\"F1\", \"RECALL\", \"PRECISION\", \"ACCURACY\"]\n",
    "    for index, metric in enumerate(METRICS):\n",
    "        tMetric = torch.tensor([m[index] for m in trainingMetrics], dtype=torch.float32)\n",
    "        vMetric = torch.tensor([m[index] for m in validationMetrics], dtype=torch.float32)\n",
    "        plotLearningCurve(epochsAxis, [(vMetric, 'Validation'), (tMetric, 'Training')], 'Epoch', metric, logger)\n",
    "\n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the training set.\n",
    "    if not model.shouldEvaluate():\n",
    "        plotConfusionMatrix(trainingLabels.numpy(), predictedTrainingLabels.numpy(), logger, \"Training\")\n",
    "        classificationReport(trainingLabels.numpy(), predictedTrainingLabels.numpy(), logger, \"Training\")\n",
    "    \n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the training set in case of an AFTER TRAINING EVALUATION.\n",
    "    else:\n",
    "        plotConfusionMatrix(evaluationLabels.numpy(), predictedEvaluationLabels.numpy(), logger, \"Training\")\n",
    "        classificationReport(evaluationLabels.numpy(), predictedEvaluationLabels.numpy(), logger, \"Training\")\n",
    "    \n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the valiation set.\n",
    "    plotConfusionMatrix(validationLabels.numpy(), predictedValidationLabels.numpy(), logger, \"Validation\")\n",
    "    classificationReport(validationLabels.numpy(), predictedValidationLabels.numpy(), logger, \"Validation\")\n",
    "    \n",
    "    # If an additional evaluation on the training set is needed.\n",
    "    if model.shouldEvaluate():\n",
    "        if logger is not None:\n",
    "            logger.log(f\"Training:\\n\"\n",
    "                       f\"Loss: {evaluationLoss:.6f} \"\n",
    "                       f\"F1: {evaluationMetrics[0]:.6f} \"\n",
    "                       f\"Recall: {evaluationMetrics[1]:.6f} \"\n",
    "                       f\"Precision: {evaluationMetrics[2]:.6f} \"\n",
    "                       f\"Accuracy: {evaluationMetrics[3]:.6f}\")\n",
    "\n",
    "        print(f\"Training:\\n\"\n",
    "              f\"Loss: {evaluationLoss:.6f} \"\n",
    "              f\"F1: {evaluationMetrics[0]:.6f} \"\n",
    "              f\"Recall: {evaluationMetrics[1]:.6f} \"\n",
    "              f\"Precision: {evaluationMetrics[2]:.6f} \"\n",
    "              f\"Accuracy: {evaluationMetrics[3]:.6f}\")\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        if logger is not None:\n",
    "            logger.log(f\"Training:\\n\"\n",
    "                       f\"Loss: {trainingLoss[-1]:.6f} \"\n",
    "                       f\"F1: {trainingMetrics[-1][0]:.6f} \"\n",
    "                       f\"Recall: {trainingMetrics[-1][1]:.6f} \"\n",
    "                       f\"Precision: {trainingMetrics[-1][2]:.6f} \"\n",
    "                       f\"Accuracy: {trainingMetrics[-1][3]:.6f}\")\n",
    "\n",
    "        print(f\"Training:\\n\"\n",
    "              f\"Loss: {trainingLoss[-1]:.6f} \"\n",
    "              f\"F1: {trainingMetrics[-1][0]:.6f} \"\n",
    "              f\"Recall: {trainingMetrics[-1][1]:.6f} \"\n",
    "              f\"Precision: {trainingMetrics[-1][2]:.6f} \"\n",
    "              f\"Accuracy: {trainingMetrics[-1][3]:.6f}\")\n",
    "    \n",
    "    if logger is not None:\n",
    "        logger.log(f\"Validation:\\n\"\n",
    "                   f\"Loss: {validationLoss[-1]:.6f} \"\n",
    "                   f\"F1: {validationMetrics[-1][0]:.6f} \"\n",
    "                   f\"Recall: {validationMetrics[-1][1]:.6f} \"\n",
    "                   f\"Precision: {validationMetrics[-1][2]:.6f} \"\n",
    "                   f\"Accuracy: {validationMetrics[-1][3]:.6f}\")\n",
    "\n",
    "    print(f\"Validation:\\n\"\n",
    "          f\"Loss: {validationLoss[-1]:.6f} \"\n",
    "          f\"F1: {validationMetrics[-1][0]:.6f} \"\n",
    "          f\"Recall: {validationMetrics[-1][1]:.6f} \"\n",
    "          f\"Precision: {validationMetrics[-1][2]:.6f} \"\n",
    "          f\"Accuracy: {validationMetrics[-1][3]:.6f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate predictions on the testing set and store them according to the provided instructions.\n",
    "    testingIDS = testingDataset.getIDS()\n",
    "    testingInstances = testingDataset.getInstances()\n",
    "    \n",
    "    # Switch the model to evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        # Forward pass.\n",
    "        logits = model(testingInstances)\n",
    "        probabilities = nn.functional.softmax(logits, dim=1)\n",
    "        testingLabels = torch.argmax(probabilities, dim=1)\n",
    "    \n",
    "    testingLabels = list(testingLabels.numpy())\n",
    "    for i in range(len(testingLabels)):\n",
    "        testingLabels[i] = Tweet.numberToClass[testingLabels[i]]\n",
    "        \n",
    "    values = list(zip(testingIDS, testingLabels))\n",
    "    dataframe = pandas.DataFrame(values, columns=['Id', 'Predicted'])\n",
    "    dataframe.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9717b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T19:03:27.847526Z",
     "iopub.status.busy": "2023-12-13T19:03:27.847125Z",
     "iopub.status.idle": "2023-12-13T19:05:16.707317Z",
     "shell.execute_reply": "2023-12-13T19:05:16.706218Z"
    },
    "papermill": {
     "duration": 108.870289,
     "end_time": "2023-12-13T19:05:16.709752",
     "exception": false,
     "start_time": "2023-12-13T19:03:27.839463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputSize = TweetDataset.dimensions\n",
    "\n",
    "model = Model(inputSize=inputSize, seed=seed)\n",
    "model.addDense(neurons=64)\n",
    "model.addRelu()\n",
    "model.addDense(neurons=64)\n",
    "model.addRelu()\n",
    "model.addDense(neurons=32)\n",
    "model.addRelu()\n",
    "\n",
    "# Output just 3 raw/unnormalized logits, one for each class as \n",
    "# Pytorch's CrossEntropyLoss applies the softmax operation by default when provided with the class indices as the target.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "model.addDense(neurons=3)\n",
    "model.build()\n",
    "\n",
    "learningRate = 0.01\n",
    "optimizers = [torch.optim.ASGD(model.parameters(), lr=learningRate),\n",
    "              torch.optim.Adam(model.parameters(), lr=learningRate),\n",
    "              torch.optim.SGD(model.parameters(), lr=learningRate),\n",
    "              torch.optim.Adagrad(model.parameters(), lr=learningRate)]\n",
    "\n",
    "optimizer = optimizers[0]\n",
    "customScheduler = CustomScheduler(torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.85))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "epochs = 92\n",
    "earlyStop = None\n",
    "\n",
    "logger = None\n",
    "\n",
    "train(model=model,\n",
    "      criterion=criterion,\n",
    "      optimizer=optimizer,\n",
    "      scheduler=customScheduler,\n",
    "      epochs=epochs,\n",
    "      earlyStop=earlyStop,\n",
    "      logger=logger)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7104041,
     "sourceId": 64789,
     "sourceType": "competition"
    },
    {
     "datasetId": 4154708,
     "sourceId": 7186549,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.043331,
   "end_time": "2023-12-13T19:05:17.950759",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-13T19:02:28.907428",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
