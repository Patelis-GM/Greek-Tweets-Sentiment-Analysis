{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3713ec95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:24.408597Z",
     "iopub.status.busy": "2023-11-19T19:38:24.408100Z",
     "iopub.status.idle": "2023-11-19T19:38:30.266268Z",
     "shell.execute_reply": "2023-11-19T19:38:30.265240Z"
    },
    "papermill": {
     "duration": 5.888428,
     "end_time": "2023-11-19T19:38:30.269602",
     "exception": false,
     "start_time": "2023-11-19T19:38:24.381174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import unicodedata\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "    classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2605c1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:30.322513Z",
     "iopub.status.busy": "2023-11-19T19:38:30.321727Z",
     "iopub.status.idle": "2023-11-19T19:38:30.328102Z",
     "shell.execute_reply": "2023-11-19T19:38:30.326907Z"
    },
    "papermill": {
     "duration": 0.035863,
     "end_time": "2023-11-19T19:38:30.330715",
     "exception": false,
     "start_time": "2023-11-19T19:38:30.294852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "HYPERLINKS_REGEX = r'(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|(www\\.[^ \\s]+)'\n",
    "HASHTAGS_REGEX = r'#\\w+'\n",
    "MENTIONS_REGEX = r'@\\w+'\n",
    "NUMERIC_REGEX = r'[0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25d6c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:30.384132Z",
     "iopub.status.busy": "2023-11-19T19:38:30.383138Z",
     "iopub.status.idle": "2023-11-19T19:38:30.392049Z",
     "shell.execute_reply": "2023-11-19T19:38:30.391030Z"
    },
    "papermill": {
     "duration": 0.039092,
     "end_time": "2023-11-19T19:38:30.394861",
     "exception": false,
     "start_time": "2023-11-19T19:38:30.355769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stopwords:\n",
    "    \"\"\"\n",
    "    A utility class designed to store all the essential information related to Greek stopwords.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.__stopwords = set()\n",
    "\n",
    "        stopwordFile = open(path, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for word in stopwordFile:\n",
    "            words.append(word)\n",
    "\n",
    "        for word in words:\n",
    "            stopword = word.replace(\"\\n\",\"\")\n",
    "            self.__stopwords.add(stopword)\n",
    "            \n",
    "        stopwordFile.close()\n",
    "\n",
    "    def have(self, word):\n",
    "        return word in self.__stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c4fe3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:30.448379Z",
     "iopub.status.busy": "2023-11-19T19:38:30.447425Z",
     "iopub.status.idle": "2023-11-19T19:38:30.462953Z",
     "shell.execute_reply": "2023-11-19T19:38:30.461977Z"
    },
    "papermill": {
     "duration": 0.045641,
     "end_time": "2023-11-19T19:38:30.465780",
     "exception": false,
     "start_time": "2023-11-19T19:38:30.420139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    \"\"\"\n",
    "    A class designed to store all the essential information related to a Tweet.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.__originalText = None\n",
    "        self.__text = None\n",
    "        self.__mentions = []\n",
    "        self.__links = []\n",
    "        self.__hashtags = []\n",
    "        self.__sentiment = None\n",
    "        self.__party = None\n",
    "        self.__id = None\n",
    "    \n",
    "    def setID(self, id):\n",
    "        self.__id = id\n",
    "        \n",
    "    def getID(self):\n",
    "        return self.__id\n",
    "        \n",
    "    def addLinks(self, links):\n",
    "        for link in links:\n",
    "            self.__links.append(link)\n",
    "    \n",
    "    def getLinks(self):\n",
    "        return self.__links\n",
    "    \n",
    "    def getTotalLinks(self):\n",
    "        return len(self.__links)\n",
    "\n",
    "    def addHashtags(self, hashtags):\n",
    "        for hashtag in hashtags:\n",
    "            self.__hashtags.append(hashtag)\n",
    "        \n",
    "    def getHashtags(self):\n",
    "        return self.__hashtags\n",
    "\n",
    "    def getTotalHashtags(self):\n",
    "        return len(self.__hashtags)\n",
    "\n",
    "    def addMentions(self, mentions):\n",
    "        for mention in mentions:\n",
    "            self.__mentions.append(mention)\n",
    "    \n",
    "    def getMentions(self):\n",
    "        return self.__mentions\n",
    "    \n",
    "    def getTotalMentions(self):\n",
    "        return len(self.__mentions)\n",
    "\n",
    "    def setParty(self, party):\n",
    "        self.__party = party\n",
    "\n",
    "    def getParty(self):\n",
    "        return self.__party\n",
    "\n",
    "    def setSentiment(self, sentiment):\n",
    "        self.__sentiment = sentiment\n",
    "\n",
    "    def getSentiment(self):\n",
    "        return self.__sentiment\n",
    "\n",
    "    def setText(self, text):\n",
    "        self.__text = text\n",
    "\n",
    "    def getText(self):\n",
    "        return self.__text\n",
    "\n",
    "    def setOriginalText(self, originalText):\n",
    "        self.__originalText = originalText\n",
    "\n",
    "    def getOriginalText(self):\n",
    "        return self.__originalText\n",
    "\n",
    "    def isPositive(self):\n",
    "        return self.__sentiment == 0\n",
    "\n",
    "    def isNegative(self):\n",
    "        return self.__sentiment == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3a4d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:30.520569Z",
     "iopub.status.busy": "2023-11-19T19:38:30.519606Z",
     "iopub.status.idle": "2023-11-19T19:38:34.284865Z",
     "shell.execute_reply": "2023-11-19T19:38:34.283107Z"
    },
    "papermill": {
     "duration": 3.795895,
     "end_time": "2023-11-19T19:38:34.288322",
     "exception": false,
     "start_time": "2023-11-19T19:38:30.492427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = {\"POSITIVE\": 0, \"NEGATIVE\": 1, \"NEUTRAL\": 2}\n",
    "reverseClasses = {0: \"POSITIVE\", 1: \"NEGATIVE\", 2: \"NEUTRAL\"}\n",
    "\n",
    "nlp = spacy.load(\"el_core_news_lg\")\n",
    "\n",
    "# Read the README section. This file is really important\n",
    "greekStopwords = Stopwords(\"Greek-Stopwords.txt\")\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "encounteredParties = dict()\n",
    "\n",
    "\n",
    "def removeAccents(token):\n",
    "    \"\"\"\n",
    "    A function designed to eliminate accents from the given token.\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        character for character in unicodedata.normalize('NFD', token) if unicodedata.category(character) != 'Mn')\n",
    "\n",
    "\n",
    "def hasSpecialCharacters(token):\n",
    "    \"\"\"\n",
    "    A function designed to check if the given token has any special characters.\n",
    "    \"\"\"\n",
    "    for character in token:\n",
    "        if not character.isalnum():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def removeSpecialCharacters(token):\n",
    "    \"\"\"\n",
    "    A function designed to eliminate special characters from the given token.\n",
    "    \"\"\"\n",
    "    refinedToken = \"\"\n",
    "    for character in token:\n",
    "        if character.isalnum():\n",
    "            refinedToken += character\n",
    "    return refinedToken\n",
    "\n",
    "\n",
    "def normalize(token):\n",
    "    \"\"\"\n",
    "    A function designed to normalize the given token.\n",
    "    \n",
    "    Normalization includes:\n",
    "    \n",
    "    - Uppercase conversion\n",
    "    \n",
    "    - Accent elimination\n",
    "    \n",
    "    - Replacing consecutive identical characters with a single character\n",
    "    \n",
    "    - Note that order matters as the normalization won't succeed with the word Αγέεεέλη\n",
    "    \"\"\"\n",
    "    token = token.upper()\n",
    "    token = removeAccents(token)\n",
    "    token = re.sub(r'(.)\\1+', r'\\1', token)\n",
    "    return token\n",
    "\n",
    "\n",
    "def processDataset(path, isTestingSet):\n",
    "    \"\"\"\n",
    "    A function designed to preprocess the given dataset.\n",
    "    The full processing routine is documented in the supplied report.    \n",
    "    \"\"\"\n",
    "    dataframe = pandas.read_csv(path, encoding=\"utf-8\")\n",
    "    tweets = []\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "\n",
    "        tweet = Tweet()\n",
    "        \n",
    "        # Store the Tweet's id\n",
    "        tweet.setID(row['New_ID'])\n",
    "        \n",
    "        # Store Tweet's party\n",
    "        tweet.setParty(row['Party'])\n",
    "\n",
    "        # Store Tweet's sentiment as long as it doesn't belong to the testing set\n",
    "        if not isTestingSet:\n",
    "            tweet.setSentiment(classes[row['Sentiment']])\n",
    "\n",
    "        # Store Tweet's original text\n",
    "        text = row['Text']\n",
    "        tweet.setOriginalText(text)\n",
    "\n",
    "        # Find all hyperlinks in the Tweet's text and store them.\n",
    "        links = re.findall(HYPERLINKS_REGEX, text)\n",
    "        links = [\"\".join(group) for group in links]\n",
    "        tweet.addLinks(links)\n",
    "\n",
    "        # Eliminate all hyperlinks from the Tweet's text\n",
    "        text = re.sub(HYPERLINKS_REGEX, \" \", text)\n",
    "\n",
    "        # Find all hashtags in the Tweet's text and store them.\n",
    "        hashtags = re.findall(HASHTAGS_REGEX, text)\n",
    "\n",
    "        # Eliminate all hashtags from the Tweet's text\n",
    "        text = re.sub(HASHTAGS_REGEX, \" \", text)\n",
    "\n",
    "        # Find all mentions in the Tweet's text and store them.\n",
    "        mentions = re.findall(MENTIONS_REGEX, text)\n",
    "        tweet.addMentions(mentions)\n",
    "\n",
    "        # Eliminate all mentions from the Tweet's text\n",
    "        text = re.sub(MENTIONS_REGEX, \" \", text)\n",
    "\n",
    "        # Eliminate all numeric characters from the Tweet's text\n",
    "        text = re.sub(NUMERIC_REGEX, \" \", text)\n",
    "\n",
    "        # Tokenize the Tweet's modified text using the ntlk TweetTokenizer\n",
    "        # Join the tokens produced and then tokenize the produced string using spaCy\n",
    "        # This decision was made after observations on how the spaCy model handles tokenization\n",
    "        tweetTokens = nlp(\" \".join(tokenizer.tokenize(text)))\n",
    "\n",
    "        refinedTokens = []\n",
    "\n",
    "        for token in tweetTokens:\n",
    "\n",
    "            # If the corresponding token is identified as a stopword, eliminate it.\n",
    "            if greekStopwords.have(normalize(token.text)):\n",
    "                continue\n",
    "\n",
    "            # If the corresponding token is a special character, eliminate it.\n",
    "            if hasSpecialCharacters(token.text):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the corresponding token\n",
    "            refinedToken = token.lemma_\n",
    "\n",
    "            # Normalize the corresponding token\n",
    "            refinedToken = normalize(refinedToken)\n",
    "\n",
    "            # Eliminate the concluding Σ character, if present, as\n",
    "            # words such as ΤΣΙΠΡΑ and ΤΣΙΠΡΑΣ share the same semantic value\n",
    "            if len(refinedToken) > 1 and refinedToken[len(refinedToken) - 1] == \"Σ\":\n",
    "                refinedToken = refinedToken[:len(refinedToken) - 1]\n",
    "\n",
    "            # Store the refined token\n",
    "            refinedTokens.append(refinedToken)\n",
    "\n",
    "        refinedHashtags = []\n",
    "\n",
    "        for hashtag in hashtags:\n",
    "\n",
    "            # Remove the # character\n",
    "            refinedHashtag = hashtag[1:]\n",
    "\n",
    "            # Normalize the corresponding hashtag\n",
    "            refinedHashtag = normalize(refinedHashtag)\n",
    "\n",
    "            # Eliminate any special characters from the corresponding hashtag\n",
    "            refinedHashtag = removeSpecialCharacters(refinedHashtag)\n",
    "\n",
    "            # Eliminate the concluding Σ character, if present, as\n",
    "            # hashtags such as #ΓΑΤΑ_ΤΕΛΟΣ and #ΓΑΤΑ_ΤΕΛΟ refer to the same topic\n",
    "            if len(refinedHashtag) > 1 and refinedHashtag[len(refinedHashtag) - 1] == \"Σ\":\n",
    "                refinedHashtag = refinedHashtag[:len(refinedHashtag) - 1]\n",
    "\n",
    "            # Store the refined hashtag in the refined tokens list\n",
    "            refinedTokens.append(refinedHashtag)\n",
    "\n",
    "            # Store the refined hashtag in the refined hashtags list\n",
    "            refinedHashtags.append(refinedHashtag)\n",
    "\n",
    "        # Unify all refined tokens into a single string and store it\n",
    "        tweet.setText(\" \".join(refinedTokens))\n",
    "\n",
    "        # Store all the Tweet's refined hashtags for potential future use\n",
    "        tweet.addHashtags(refinedHashtags)\n",
    "        tweets.append(tweet)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def prepareDataset(tweets, vectorizer, useParty, useLinks, isTrainingSet):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    partyCounter = 1\n",
    "\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        # Store the Tweet's id\n",
    "        ids.append(tweet.getID())\n",
    "        \n",
    "        # Store the Tweet's text and sentiment\n",
    "        corpus.append(tweet.getText())\n",
    "        labels.append(tweet.getSentiment())\n",
    "\n",
    "        # In the case of the training set, store all encountered parties\n",
    "        # and assign a unique numerical identifier to each one of them\n",
    "        if isTrainingSet and tweet.getParty() not in encounteredParties.keys():\n",
    "            encounteredParties[tweet.getParty()] = partyCounter\n",
    "            partyCounter += 1\n",
    "\n",
    "    if isTrainingSet:\n",
    "        featureMatrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    else:\n",
    "        featureMatrix = vectorizer.transform(corpus)\n",
    "\n",
    "    instances = featureMatrix.toarray()\n",
    "\n",
    "    # In the case where the Tweet's party is included as a feature\n",
    "    # if its party has been encountered before set the feature's value to the\n",
    "    # unique numerical identifier associated with the Tweet's party, otherwise set it to 0\n",
    "    if useParty:\n",
    "        parties = []\n",
    "        for tweet in tweets:\n",
    "            if tweet.getParty() in encounteredParties.keys():\n",
    "                parties.append(encounteredParties[tweet.getParty()])\n",
    "            else:\n",
    "                parties.append(0)\n",
    "\n",
    "        instances = numpy.hstack((instances, numpy.array(parties).reshape(-1, 1)))\n",
    "\n",
    "    # In the case where the presence of hyperlinks in a Tweet is included as a feature\n",
    "    # if the Tweet has at least one hyperlink set the feature's value to 1, otherwise set it to 0\n",
    "    if useLinks:\n",
    "        links = []\n",
    "        for tweet in tweets:\n",
    "            if tweet.getTotalLinks() > 0:\n",
    "                links.append(1)\n",
    "            else:\n",
    "                links.append(0)\n",
    "\n",
    "        instances = numpy.hstack((instances, numpy.array(links).reshape(-1, 1)))\n",
    "\n",
    "    return instances, labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56cf312a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:34.343146Z",
     "iopub.status.busy": "2023-11-19T19:38:34.342334Z",
     "iopub.status.idle": "2023-11-19T19:38:34.349749Z",
     "shell.execute_reply": "2023-11-19T19:38:34.348749Z"
    },
    "papermill": {
     "duration": 0.037531,
     "end_time": "2023-11-19T19:38:34.352285",
     "exception": false,
     "start_time": "2023-11-19T19:38:34.314754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateModel(model, instances, labels, average):\n",
    "    \"\"\"\n",
    "    A function designed to evaluate a model's F1, Recall, Precision and Accuracy scores\n",
    "    \"\"\"\n",
    "    predictions = model.predict(instances)\n",
    "    f1 = f1_score(labels, predictions, average=average)\n",
    "    recall = recall_score(labels, predictions, average=average)\n",
    "    precision = precision_score(labels, predictions, average=average)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return f1, recall, precision, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74250c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:38:34.405861Z",
     "iopub.status.busy": "2023-11-19T19:38:34.405375Z",
     "iopub.status.idle": "2023-11-19T19:49:44.698091Z",
     "shell.execute_reply": "2023-11-19T19:49:44.696753Z"
    },
    "papermill": {
     "duration": 670.324438,
     "end_time": "2023-11-19T19:49:44.702159",
     "exception": false,
     "start_time": "2023-11-19T19:38:34.377721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingTweets = processDataset(path=\"Data/train_set.csv\", isTestingSet=False)\n",
    "validationTweets = processDataset(path=\"Data/valid_set.csv\", isTestingSet=False)\n",
    "testingTweets = processDataset(path=\"Data/test_set.csv\", isTestingSet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d37ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:49:44.758023Z",
     "iopub.status.busy": "2023-11-19T19:49:44.757538Z",
     "iopub.status.idle": "2023-11-19T19:52:47.662623Z",
     "shell.execute_reply": "2023-11-19T19:52:47.660821Z"
    },
    "papermill": {
     "duration": 182.937585,
     "end_time": "2023-11-19T19:52:47.667522",
     "exception": false,
     "start_time": "2023-11-19T19:49:44.729937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "useParty = True\n",
    "useLinks = False\n",
    "bow = True\n",
    "maxFeatures = 1600\n",
    "\n",
    "# IF - ELSE control structure to experiment with both BOW and TF-IDF\n",
    "# Both vectorizers come with a custom tokenizer as I don't trust sklearn enough\n",
    "if bow:\n",
    "    vectorizer = CountVectorizer(max_features=maxFeatures,\n",
    "                                 tokenizer=lambda text: text.split(), lowercase=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_features=maxFeatures,\n",
    "                                 tokenizer=lambda text: text.split(), lowercase=False)\n",
    "\n",
    "trainingInstances, trainingLabels, _ = prepareDataset(tweets=trainingTweets,\n",
    "                                                      vectorizer=vectorizer,\n",
    "                                                      useParty=useParty,\n",
    "                                                      useLinks=useLinks,\n",
    "                                                      isTrainingSet=True)\n",
    "\n",
    "validationInstances, validationLabels, _ = prepareDataset(tweets=validationTweets,\n",
    "                                                          vectorizer=vectorizer,\n",
    "                                                          useParty=useParty,\n",
    "                                                          useLinks=useLinks,\n",
    "                                                          isTrainingSet=False)\n",
    "\n",
    "testingInstances, _, testingIds = prepareDataset(tweets=testingTweets,\n",
    "                                                 vectorizer=vectorizer,\n",
    "                                                 useParty=useParty,\n",
    "                                                 useLinks=useLinks,\n",
    "                                                 isTrainingSet=False)\n",
    "\n",
    "\n",
    "def printEssentials():\n",
    "    \"\"\"\n",
    "    Utility function to print the essential\n",
    "    information of each experiment\n",
    "    \"\"\"\n",
    "    print(\"# VECTORIZER #\")\n",
    "    if bow:\n",
    "        print(\"vectorizer: BOW\")\n",
    "    else:\n",
    "        print(\"vectorizer: TF-IDF\")\n",
    "    print(f\"useParty: {useParty}\")\n",
    "    print(f\"useLinks: {useLinks}\")\n",
    "    print(f\"maxFeatures: {maxFeatures}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Model parameters as suggested by Optuna\n",
    "iterations = 10000\n",
    "C = 0.00252187198163273\n",
    "solver = \"saga\"\n",
    "penalty = \"l2\"\n",
    "multiClass = \"multinomial\"\n",
    "\n",
    "# Evaluation parameter\n",
    "average = \"weighted\"\n",
    "\n",
    "# Set a random state to ensure reproducibility\n",
    "randomState = 420\n",
    "\n",
    "\n",
    "def modelScaling(trainingInstances, trainingLabels, validationInstances, validationLabels, testingInstances,\n",
    "                 testingIds):\n",
    "    sampleSize = []\n",
    "    trainingMetrics = []\n",
    "    validationMetrics = []\n",
    "    model = None\n",
    "    start = 0.1\n",
    "    end = 1.1\n",
    "    step = 0.1\n",
    "    steps = numpy.arange(start, end, step)\n",
    "    sentimentLabels = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "\n",
    "    for i in steps:\n",
    "\n",
    "        print(f\"Progress: {(i * 100):.2f} %\")\n",
    "\n",
    "        model = LogisticRegression(max_iter=iterations,\n",
    "                                   penalty=penalty, C=C,\n",
    "                                   solver=solver, multi_class=multiClass)\n",
    "\n",
    "        # Train the model on the complete training set or on a (i * 100) percentage of it  \n",
    "        if i >= 1.0:\n",
    "            X, Y = trainingInstances, trainingLabels\n",
    "        else:\n",
    "            X, _, Y, _ = train_test_split(trainingInstances, trainingLabels,\n",
    "                                          train_size=i, random_state=randomState)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # Evaluate the model on the training data\n",
    "        metrics = evaluateModel(model, X, Y, average=average)\n",
    "        trainingMetrics.append(metrics)\n",
    "\n",
    "        # Evaluate the model on the validation data\n",
    "        metrics = evaluateModel(model, validationInstances, validationLabels, average=average)\n",
    "        validationMetrics.append(metrics)\n",
    "\n",
    "        sampleSize.append(i)\n",
    "\n",
    "    # Evaluate the fully trained model on the validation set\n",
    "    valLogMetrics = evaluateModel(model, validationInstances, validationLabels, average=average)\n",
    "\n",
    "    # Evaluate the fully trained model on the training set\n",
    "    trnLogMetrics = evaluateModel(model, trainingInstances, trainingLabels, average=average)\n",
    "\n",
    "    # Plot each of the following metrics\n",
    "    METRICS = [\"F1\", \"RECALL\", \"PRECISION\", \"ACCURACY\"]\n",
    "    for i in range(len(METRICS)):\n",
    "        metric = METRICS[i]\n",
    "        print(f\"Training - {metric}: {trnLogMetrics[i]}\")\n",
    "        print(f\"Validation - {metric}: {valLogMetrics[i]}\")\n",
    "        print()\n",
    "        trainingMetric = [value[i] for value in trainingMetrics]\n",
    "        validationMetric = [value[i] for value in validationMetrics]\n",
    "        pyplot.plot(sampleSize, trainingMetric, label=\"Training\", marker='o')\n",
    "        pyplot.plot(sampleSize, validationMetric, label=\"Validation\", marker='x')\n",
    "        pyplot.xlabel('Data Percentage')\n",
    "        pyplot.ylabel(metric)\n",
    "        pyplot.grid(True)\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "    # Calculate, display and save the Confusion Matrix related to the validation set\n",
    "    predictions = model.predict(validationInstances)\n",
    "    confusionMatrix = confusion_matrix(validationLabels, predictions, labels=[0, 1, 2])\n",
    "    confusionMatrixDisplay = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=sentimentLabels)\n",
    "    confusionMatrixDisplay.plot()\n",
    "    pyplot.show()\n",
    "\n",
    "    # Calculate and print the classification report related to the training set\n",
    "    classificationReport = classification_report(validationLabels, predictions, target_names=sentimentLabels)\n",
    "    print(\"# Validation - Classification Report #\")\n",
    "    print(classificationReport)\n",
    "    print()\n",
    "\n",
    "    # Calculate, display and save the Confusion Matrix related to the training set\n",
    "    predictions = model.predict(trainingInstances)\n",
    "    confusionMatrix = confusion_matrix(trainingLabels, predictions, labels=[0, 1, 2])\n",
    "    confusionMatrixDisplay = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=sentimentLabels)\n",
    "    confusionMatrixDisplay.plot()\n",
    "    pyplot.show()\n",
    "\n",
    "    # Calculate and print the classification report related to the training set\n",
    "    classificationReport = classification_report(trainingLabels, predictions, target_names=sentimentLabels)\n",
    "    print(\"# Training - Classification Report #\")\n",
    "    print(classificationReport)\n",
    "    print()\n",
    "\n",
    "    # Make the predictions on the testing set and save them as instructed\n",
    "    testPredictions = model.predict(testingInstances)\n",
    "    testPredictions = [reverseClasses[prediction] for prediction in testPredictions]\n",
    "    values = list(zip(testingIds, testPredictions))\n",
    "    dataframe = pandas.DataFrame(values, columns=['Id', 'Predicted'])\n",
    "    dataframe.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "# Print each parameter used for the corresponding experiment\n",
    "printEssentials()\n",
    "print(f\"# MODEL-SCALING #\")\n",
    "print(f\"iterations: {iterations}\")\n",
    "print(f\"penalty: {penalty}\")\n",
    "print(f\"C: {C}\")\n",
    "print(f\"solver: {solver}\")\n",
    "print(f\"multiClass: {multiClass}\")\n",
    "print(f\"average: {average}\")\n",
    "print(f\"randomState: {randomState}\")\n",
    "print()\n",
    "\n",
    "modelScaling(trainingInstances=trainingInstances,\n",
    "             trainingLabels=trainingLabels,\n",
    "             validationInstances=validationInstances,\n",
    "             validationLabels=validationLabels,\n",
    "             testingInstances=testingInstances,\n",
    "             testingIds=testingIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d2e0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T19:52:47.734201Z",
     "iopub.status.busy": "2023-11-19T19:52:47.733685Z",
     "iopub.status.idle": "2023-11-19T19:52:47.741390Z",
     "shell.execute_reply": "2023-11-19T19:52:47.740161Z"
    },
    "papermill": {
     "duration": 0.044125,
     "end_time": "2023-11-19T19:52:47.743803",
     "exception": false,
     "start_time": "2023-11-19T19:52:47.699678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def study(trial):\n",
    "\n",
    "    # Definition of the BOW vectorizer search space\n",
    "    useParty = trial.suggest_categorical('useParty', [True, False])\n",
    "    useLinks = trial.suggest_categorical('useLinks', [True, False])\n",
    "    maxFeatures = trial.suggest_int(name=\"Features\", low=1500, high=2000, step=100)\n",
    "    vectorizer = CountVectorizer(max_features=maxFeatures,\n",
    "                                 tokenizer=lambda text: text.split(), lowercase=False)\n",
    "\n",
    "    trainingInstances, trainingLabels, _ = prepareDataset(tweets=trainingTweets,\n",
    "                                                          vectorizer=vectorizer,\n",
    "                                                          useParty=useParty,\n",
    "                                                          useLinks=useLinks,\n",
    "                                                          isTrainingSet=True)\n",
    "\n",
    "    validationInstances, validationLabels, _ = prepareDataset(tweets=validationTweets,\n",
    "                                                              vectorizer=vectorizer,\n",
    "                                                              useParty=useParty,\n",
    "                                                              useLinks=useLinks,\n",
    "                                                              isTrainingSet=False)\n",
    "\n",
    "    iterations = 10000\n",
    "\n",
    "    # Definition of the Logistic Regression model search space\n",
    "    C = trial.suggest_float('C', low=1e-4, high=1e-2, log=True)\n",
    "    multiClass = trial.suggest_categorical('multi_class', ['multinomial', 'ovr'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1'])\n",
    "\n",
    "    model = LogisticRegression(max_iter=iterations,\n",
    "                               penalty=penalty, C=C,\n",
    "                               solver=\"saga\", multi_class=multiClass)\n",
    "    model.fit(trainingInstances, trainingLabels)\n",
    "\n",
    "    predictions = model.predict(validationInstances)\n",
    "    valF1 = f1_score(validationLabels, predictions, average=\"weighted\")\n",
    "\n",
    "    predictions = model.predict(trainingInstances)\n",
    "    trainF1 = f1_score(trainingLabels, predictions, average=\"weighted\")\n",
    "\n",
    "    threshold = 0.01\n",
    "\n",
    "    if trainF1 - valF1 <= threshold:\n",
    "        return valF1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize()\n",
    "parameters = study.best_params\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6886092,
     "sourceId": 62857,
     "sourceType": "competition"
    },
    {
     "datasetId": 4013884,
     "sourceId": 6984102,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 910.99216,
   "end_time": "2023-11-19T19:52:50.110969",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-19T19:37:39.118809",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
