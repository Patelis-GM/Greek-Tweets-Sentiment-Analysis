{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfad2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:12:53.911889Z",
     "iopub.status.busy": "2024-02-05T18:12:53.911651Z",
     "iopub.status.idle": "2024-02-05T18:13:00.465810Z",
     "shell.execute_reply": "2024-02-05T18:13:00.464861Z"
    },
    "papermill": {
     "duration": 6.566342,
     "end_time": "2024-02-05T18:13:00.468382",
     "exception": false,
     "start_time": "2024-02-05T18:12:53.902040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import unicodedata\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59598124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.493210Z",
     "iopub.status.busy": "2024-02-05T18:13:00.492253Z",
     "iopub.status.idle": "2024-02-05T18:13:00.580534Z",
     "shell.execute_reply": "2024-02-05T18:13:00.579548Z"
    },
    "papermill": {
     "duration": 0.103033,
     "end_time": "2024-02-05T18:13:00.583019",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.479986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30acc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.607998Z",
     "iopub.status.busy": "2024-02-05T18:13:00.607636Z",
     "iopub.status.idle": "2024-02-05T18:13:00.618425Z",
     "shell.execute_reply": "2024-02-05T18:13:00.617575Z"
    },
    "papermill": {
     "duration": 0.024157,
     "end_time": "2024-02-05T18:13:00.620600",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.596443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Utility class created to thoroughly document the outcomes of each experiment.\n",
    "        \"\"\"\n",
    "        currentDirectory = os.getcwd()\n",
    "        directories = []\n",
    "\n",
    "        try:\n",
    "            os.mkdir(f\"{currentDirectory}\\\\{name}\")\n",
    "        except OSError as osError:\n",
    "            print(f\"Logger: {osError}\")\n",
    "\n",
    "        currentDirectory = f\"{currentDirectory}\\\\{name}\"\n",
    "\n",
    "        for dirName in os.listdir(currentDirectory):\n",
    "            if \"EXPERIMENT\" in dirName:\n",
    "                directories.append(int(dirName[dirName.index(\"-\") + 1:]))\n",
    "\n",
    "        if len(directories) == 0:\n",
    "            self.__directory = f\"{currentDirectory}\\\\EXPERIMENT-0\"\n",
    "            os.mkdir(self.__directory)\n",
    "\n",
    "        else:\n",
    "            directories.sort()\n",
    "            latestDirectory = directories[-1]\n",
    "            self.__directory = f\"{currentDirectory}\\\\EXPERIMENT-{latestDirectory + 1}\"\n",
    "            os.mkdir(self.__directory)\n",
    "\n",
    "        self.__log = open(f\"{self.__directory}\\\\LOG.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    def log(self, toLog, newLine=True, title=None):\n",
    "        if newLine:\n",
    "            toLog += \"\\n\"\n",
    "        if title is not None:\n",
    "            self.__log.write(f\"{title}\\n\")\n",
    "        self.__log.write(toLog)\n",
    "        self.__log.flush()\n",
    "\n",
    "    def line(self):\n",
    "        self.__log.write(\"\\n\\n\")\n",
    "        self.__log.flush()\n",
    "\n",
    "    def directory(self):\n",
    "        return self.__directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36c24a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.645345Z",
     "iopub.status.busy": "2024-02-05T18:13:00.644764Z",
     "iopub.status.idle": "2024-02-05T18:13:00.654308Z",
     "shell.execute_reply": "2024-02-05T18:13:00.653310Z"
    },
    "papermill": {
     "duration": 0.023176,
     "end_time": "2024-02-05T18:13:00.656552",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.633376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \n",
    "    # Utility token to ensure that every Tweet has at least 1 token.\n",
    "    END = \"<END>\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to the word embeddings.\n",
    "        \"\"\"\n",
    "        self.__embeddings = dict()\n",
    "        self.__dimensions = -1\n",
    "        file = open(path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        for embedding in file:\n",
    "            vector = []\n",
    "\n",
    "            tokens = embedding.replace(\"\\n\", \"\").split()\n",
    "\n",
    "            for i in range(1, len(tokens)):\n",
    "                vector.append(float(tokens[i]))\n",
    "\n",
    "            self.__embeddings[tokens[0]] = vector\n",
    "\n",
    "            if self.__dimensions == -1:\n",
    "                self.__dimensions = len(tokens) - 1\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "        # Generate the embedding of the <END> token.\n",
    "        self.__embeddings[Embeddings.END] = [0.05 for _ in range(self.__dimensions)]\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.__embeddings.keys()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item in self.__embeddings.keys():\n",
    "            return self.__embeddings[item]\n",
    "        else:\n",
    "            return [0.0 for _ in range(self.__dimensions)]\n",
    "\n",
    "    def dimensions(self):\n",
    "        return self.__dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76d7f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.675768Z",
     "iopub.status.busy": "2024-02-05T18:13:00.675506Z",
     "iopub.status.idle": "2024-02-05T18:13:00.687271Z",
     "shell.execute_reply": "2024-02-05T18:13:00.686444Z"
    },
    "papermill": {
     "duration": 0.024103,
     "end_time": "2024-02-05T18:13:00.689490",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.665387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    classToNumber = {\"POSITIVE\": 0, \"NEGATIVE\": 1, \"NEUTRAL\": 2}\n",
    "    numberToClass = {0: \"POSITIVE\", 1: \"NEGATIVE\", 2: \"NEUTRAL\"}\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to a tweet.\n",
    "        \"\"\"\n",
    "        self.__text = None\n",
    "        self.__mentions = []\n",
    "        self.__links = []\n",
    "        self.__hashtags = []\n",
    "        self.__sentiment = None\n",
    "        self.__party = None\n",
    "        self.__id = None\n",
    "\n",
    "    def setID(self, id):\n",
    "        self.__id = id\n",
    "\n",
    "    def getID(self):\n",
    "        return self.__id\n",
    "\n",
    "    def addLinks(self, links):\n",
    "        for link in links:\n",
    "            self.__links.append(link)\n",
    "\n",
    "    def getLinks(self):\n",
    "        return self.__links\n",
    "\n",
    "    def getTotalLinks(self):\n",
    "        return len(self.__links)\n",
    "\n",
    "    def addHashtags(self, hashtags):\n",
    "        for hashtag in hashtags:\n",
    "            self.__hashtags.append(hashtag)\n",
    "\n",
    "    def getHashtags(self):\n",
    "        return self.__hashtags\n",
    "\n",
    "    def getTotalHashtags(self):\n",
    "        return len(self.__hashtags)\n",
    "\n",
    "    def addMentions(self, mentions):\n",
    "        for mention in mentions:\n",
    "            self.__mentions.append(mention)\n",
    "\n",
    "    def getMentions(self):\n",
    "        return self.__mentions\n",
    "\n",
    "    def getTotalMentions(self):\n",
    "        return len(self.__mentions)\n",
    "\n",
    "    def setParty(self, party):\n",
    "        self.__party = party\n",
    "\n",
    "    def getParty(self):\n",
    "        return self.__party\n",
    "\n",
    "    def setSentiment(self, sentiment):\n",
    "        self.__sentiment = sentiment\n",
    "\n",
    "    def getSentiment(self):\n",
    "        return self.__sentiment\n",
    "\n",
    "    def setText(self, text):\n",
    "        self.__text = text\n",
    "\n",
    "    def getText(self):\n",
    "        return self.__text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0906f9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.716219Z",
     "iopub.status.busy": "2024-02-05T18:13:00.715962Z",
     "iopub.status.idle": "2024-02-05T18:13:00.722097Z",
     "shell.execute_reply": "2024-02-05T18:13:00.721201Z"
    },
    "papermill": {
     "duration": 0.02166,
     "end_time": "2024-02-05T18:13:00.724073",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.702413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stopwords:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Utility class that stores all the essential information related to Greek stopwords.\n",
    "        \"\"\"\n",
    "        self.__stopwords = set()\n",
    "\n",
    "        stopwordFile = open(path, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for word in stopwordFile:\n",
    "            words.append(word)\n",
    "\n",
    "        for word in words:\n",
    "            stopword = word.replace(\"\\n\", \"\")\n",
    "            self.__stopwords.add(stopword)\n",
    "\n",
    "        stopwordFile.close()\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.__stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b74f4f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.743118Z",
     "iopub.status.busy": "2024-02-05T18:13:00.742823Z",
     "iopub.status.idle": "2024-02-05T18:13:00.798114Z",
     "shell.execute_reply": "2024-02-05T18:13:00.797290Z"
    },
    "papermill": {
     "duration": 0.067679,
     "end_time": "2024-02-05T18:13:00.800411",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.732732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the Greek stemmer based on the paper of Georgios Ntais which can be found at the following link https://people.dsv.su.se/~hercules/papers/Ntais_greek_stemmer_thesis_final.pdf\n",
    "\"\"\"\n",
    "\n",
    "class Stemmer:\n",
    "\n",
    "    cases = dict()\n",
    "    cases[\"ΦΑΓΙΑ\"] = \"ΦΑ\"\n",
    "    cases[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"\n",
    "    cases[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"\n",
    "    cases[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"\n",
    "    cases[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"\n",
    "    cases[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"\n",
    "    cases[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"\n",
    "    cases[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"\n",
    "    cases[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"\n",
    "    cases[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"\n",
    "    cases[\"ΣΟΓΙΑ\"] = \"ΣΟ\"\n",
    "    cases[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"\n",
    "    cases[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"\n",
    "    cases[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"\n",
    "    cases[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"\n",
    "    cases[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"\n",
    "    cases[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"\n",
    "    cases[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"\n",
    "    cases[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"\n",
    "    cases[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"\n",
    "    cases[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"\n",
    "    cases[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"\n",
    "    cases[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"\n",
    "    cases[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"\n",
    "    cases[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"\n",
    "    cases[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"\n",
    "    cases[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"\n",
    "    cases[\"ΦΩΣ\"] = \"ΦΩ\"\n",
    "    cases[\"ΦΩΤΟΣ\"] = \"ΦΩ\"\n",
    "    cases[\"ΦΩΤΑ\"] = \"ΦΩ\"\n",
    "    cases[\"ΦΩΤΩΝ\"] = \"ΦΩ\"\n",
    "    cases[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"\n",
    "    cases[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"\n",
    "    cases[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"\n",
    "    cases[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"\n",
    "    cases[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"\n",
    "    cases[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"\n",
    "    cases[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"\n",
    "    cases[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\"\n",
    "    vowels = \"[ΑΕΗΙΟΥΩ]\"\n",
    "    refinedVowels = \"[ΑΕΗΙΟΩ]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def stemWord(w: str, exceptions: dict = None):\n",
    "        stem = None\n",
    "        suffix = None\n",
    "        test1 = True\n",
    "\n",
    "        if exceptions is not None and w in exceptions.keys():\n",
    "            return exceptions[w]\n",
    "\n",
    "        if len(w) < 4:\n",
    "            return w\n",
    "\n",
    "        pattern = None\n",
    "        pattern2 = None\n",
    "        pattern3 = None\n",
    "        pattern4 = None\n",
    "\n",
    "        # Step1\n",
    "        pattern = re.compile(\n",
    "            r\"(.*)(ΦΑΓΙΑ|ΦΑΓΙΟΥ|ΦΑΓΙΩΝ|ΣΚΑΓΙΑ|ΣΚΑΓΙΟΥ|ΣΚΑΓΙΩΝ|ΟΛΟΓΙΟΥ|ΟΛΟΓΙΑ|ΟΛΟΓΙΩΝ|ΣΟΓΙΟΥ|ΣΟΓΙΑ|ΣΟΓΙΩΝ|ΤΑΤΟΓΙΑ|ΤΑΤΟΓΙΟΥ|ΤΑΤΟΓΙΩΝ|ΚΡΕΑΣ|ΚΡΕΑΤΟΣ|ΚΡΕΑΤΑ|ΚΡΕΑΤΩΝ|ΠΕΡΑΣ|ΠΕΡΑΤΟΣ|ΠΕΡΑΤΑ|ΠΕΡΑΤΩΝ|ΤΕΡΑΣ|ΤΕΡΑΤΟΣ|ΤΕΡΑΤΑ|ΤΕΡΑΤΩΝ|ΦΩΣ|ΦΩΤΟΣ|ΦΩΤΑ|ΦΩΤΩΝ|ΚΑΘΕΣΤΩΣ|ΚΑΘΕΣΤΩΤΟΣ|ΚΑΘΕΣΤΩΤΑ|ΚΑΘΕΣΤΩΤΩΝ|ΓΕΓΟΝΟΣ|ΓΕΓΟΝΟΤΟΣ|ΓΕΓΟΝΟΤΑ|ΓΕΓΟΝΟΤΩΝ)$\")\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            suffix = fp[1]\n",
    "            w = stem + Stemmer.cases[suffix]\n",
    "            test1 = False\n",
    "\n",
    "        # Step 2a\n",
    "        pattern = re.compile(r\"^(.+?)(ΑΔΕΣ|ΑΔΩΝ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            reg1 = re.compile(r\"(ΟΚ|ΜΑΜ|ΜΑΝ|ΜΠΑΜΠ|ΠΑΤΕΡ|ΓΙΑΓΙ|ΝΤΑΝΤ|ΚΥΡ|ΘΕΙ|ΠΕΘΕΡ)$\")\n",
    "\n",
    "            if not reg1.match(w):\n",
    "                w = w + \"ΑΔ\"\n",
    "\n",
    "        # Step 2b\n",
    "        pattern2 = re.compile(r\"^(.+?)(ΕΔΕΣ|ΕΔΩΝ)$\")\n",
    "        if pattern2.match(w):\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            except2 = re.compile(r\"(ΟΠ|ΙΠ|ΕΜΠ|ΥΠ|ΓΗΠ|ΔΑΠ|ΚΡΑΣΠ|ΜΙΛ)$\")\n",
    "            if except2.match(w):\n",
    "                w = w + \"ΕΔ\"\n",
    "\n",
    "        # Step 2c\n",
    "        pattern3 = re.compile(r\"^(.+?)(ΟΥΔΕΣ|ΟΥΔΩΝ)$\")\n",
    "        if pattern3.match(w):\n",
    "            fp = pattern3.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            except3 = re.compile(r\"(ΑΡΚ|ΚΑΛΙΑΚ|ΠΕΤΑΛ|ΛΙΧ|ΠΛΕΞ|ΣΚ|Σ|ΦΛ|ΦΡ|ΒΕΛ|ΛΟΥΛ|ΧΝ|ΣΠ|ΤΡΑΓ|ΦΕ)$\")\n",
    "            if except3.match(w):\n",
    "                w = w + \"ΟΥΔ\"\n",
    "\n",
    "        # Step 2d\n",
    "        pattern4 = re.compile(\"^(.+?)(ΕΩΣ|ΕΩΝ)$\")\n",
    "        if pattern4.match(w):\n",
    "            fp = pattern4.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except4 = re.compile(r\"^(Θ|Δ|ΕΛ|ΓΑΛ|Ν|Π|ΙΔ|ΠΑΡ)$\")\n",
    "            if except4.match(w):\n",
    "                w = w + \"Ε\"\n",
    "\n",
    "        # Step 3\n",
    "        pattern = re.compile(r\"^(.+?)(ΙΑ|ΙΟΥ|ΙΩΝ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            pattern2 = re.compile(Stemmer.vowels + \"$\")\n",
    "            test1 = False\n",
    "            if pattern2.match(w):\n",
    "                w = stem + \"Ι\"\n",
    "\n",
    "        # Step 4\n",
    "        pattern = re.compile(r\"^(.+?)(ΙΚΑ|ΙΚΟ|ΙΚΟΥ|ΙΚΩΝ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            pattern2 = re.compile(Stemmer.vowels + \"$\")\n",
    "            except5 = re.compile(\n",
    "                r\"^(ΑΛ|ΑΔ|ΕΝΔ|ΑΜΑΝ|ΑΜΜΟΧΑΛ|ΗΘ|ΑΝΗΘ|ΑΝΤΙΔ|ΦΥΣ|ΒΡΩΜ|ΓΕΡ|ΕΞΩΔ|ΚΑΛΠ|ΚΑΛΛΙΝ|ΚΑΤΑΔ|ΜΟΥΛ|ΜΠΑΝ|ΜΠΑΓΙΑΤ|ΜΠΟΛ|ΜΠΟΣ|ΝΙΤ|ΞΙΚ|ΣΥΝΟΜΗΛ|ΠΕΤΣ|ΠΙΤΣ|ΠΙΚΑΝΤ|ΠΛΙΑΤΣ|ΠΟΣΤΕΛΝ|ΠΡΩΤΟΔ|ΣΕΡΤ|ΣΥΝΑΔ|ΤΣΑΜ|ΥΠΟΔ|ΦΙΛΟΝ|ΦΥΛΟΔ|ΧΑΣ)$\")\n",
    "            if except5.match(w) or pattern2.match(w):\n",
    "                w = w + \"ΙΚ\"\n",
    "\n",
    "        # step 5a\n",
    "        pattern = re.compile(r\"^(.+?)(ΑΜΕ)$\")\n",
    "        pattern2 = re.compile(r\"^(.+?)(ΑΓΑΜΕ|ΗΣΑΜΕ|ΟΥΣΑΜΕ|ΗΚΑΜΕ|ΗΘΗΚΑΜΕ)$\")\n",
    "        if w == \"ΑΓΑΜΕ\":\n",
    "            w = \"ΑΓΑΜ\"\n",
    "\n",
    "        if pattern2.match(w):\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except6 = re.compile(r\"^(ΑΝΑΠ|ΑΠΟΘ|ΑΠΟΚ|ΑΠΟΣΤ|ΒΟΥΒ|ΞΕΘ|ΟΥΛ|ΠΕΘ|ΠΙΚΡ|ΠΟΤ|ΣΙΧ|Χ)$\")\n",
    "            if except6.match(w):\n",
    "                w = w + \"ΑΜ\"\n",
    "\n",
    "        # Step 5b\n",
    "        pattern2 = re.compile(r\"^(.+?)(ΑΝΕ)$\")\n",
    "        pattern3 = re.compile(r\"^(.+?)(ΑΓΑΝΕ|ΗΣΑΝΕ|ΟΥΣΑΝΕ|ΙΟΝΤΑΝΕ|ΙΟΤΑΝΕ|ΙΟΥΝΤΑΝΕ|ΟΝΤΑΝΕ|ΟΤΑΝΕ|ΟΥΝΤΑΝΕ|ΗΚΑΝΕ|ΗΘΗΚΑΝΕ)$\")\n",
    "        if pattern3.match(w):\n",
    "            fp = pattern3.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            pattern3 = re.compile(r\"^(ΤΡ|ΤΣ)$\")\n",
    "            if pattern3.match(w):\n",
    "                w = w + \"ΑΓΑΝ\"\n",
    "\n",
    "        if pattern2.match(w):\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            pattern2 = re.compile(Stemmer.refinedVowels + \"$\")\n",
    "            except7 = re.compile(\n",
    "                r\"^(ΒΕΤΕΡ|ΒΟΥΛΚ|ΒΡΑΧΜ|Γ|ΔΡΑΔΟΥΜ|Θ|ΚΑΛΠΟΥΖ|ΚΑΣΤΕΛ|ΚΟΡΜΟΡ|ΛΑΟΠΛ|ΜΩΑΜΕΘ|Μ|ΜΟΥΣΟΥΛΜ|Ν|ΟΥΛ|Π|ΠΕΛΕΚ|ΠΛ|ΠΟΛΙΣ|ΠΟΡΤΟΛ|ΣΑΡΑΚΑΤΣ|ΣΟΥΛΤ|ΤΣΑΡΛΑΤ|ΟΡΦ|ΤΣΙΓΓ|ΤΣΟΠ|ΦΩΤΟΣΤΕΦ|Χ|ΨΥΧΟΠΛ|ΑΓ|ΟΡΦ|ΓΑΛ|ΓΕΡ|ΔΕΚ|ΔΙΠΛ|ΑΜΕΡΙΚΑΝ|ΟΥΡ|ΠΙΘ|ΠΟΥΡΙΤ|Σ|ΖΩΝΤ|ΙΚ|ΚΑΣΤ|ΚΟΠ|ΛΙΧ|ΛΟΥΘΗΡ|ΜΑΙΝΤ|ΜΕΛ|ΣΙΓ|ΣΠ|ΣΤΕΓ|ΤΡΑΓ|ΤΣΑΓ|Φ|ΕΡ|ΑΔΑΠ|ΑΘΙΓΓ|ΑΜΗΧ|ΑΝΙΚ|ΑΝΟΡΓ|ΑΠΗΓ|ΑΠΙΘ|ΑΤΣΙΓΓ|ΒΑΣ|ΒΑΣΚ|ΒΑΘΥΓΑΛ|ΒΙΟΜΗΧ|ΒΡΑΧΥΚ|ΔΙΑΤ|ΔΙΑΦ|ΕΝΟΡΓ|ΘΥΣ|ΚΑΠΝΟΒΙΟΜΗΧ|ΚΑΤΑΓΑΛ|ΚΛΙΒ|ΚΟΙΛΑΡΦ|ΛΙΒ|ΜΕΓΛΟΒΙΟΜΗΧ|ΜΙΚΡΟΒΙΟΜΗΧ|ΝΤΑΒ|ΞΗΡΟΚΛΙΒ|ΟΛΙΓΟΔΑΜ|ΟΛΟΓΑΛ|ΠΕΝΤΑΡΦ|ΠΕΡΗΦ|ΠΕΡΙΤΡ|ΠΛΑΤ|ΠΟΛΥΔΑΠ|ΠΟΛΥΜΗΧ|ΣΤΕΦ|ΤΑΒ|ΤΕΤ|ΥΠΕΡΗΦ|ΥΠΟΚΟΠ|ΧΑΜΗΛΟΔΑΠ|ΨΗΛΟΤΑΒ)$\")\n",
    "            if (pattern2.match(w)) or (except7.match(w)):\n",
    "                w = w + \"ΑΝ\"\n",
    "\n",
    "        # //Step 5c\n",
    "        pattern3 = re.compile(r\"^(.+?)(ΕΤΕ)$\")\n",
    "        pattern4 = re.compile(r\"^(.+?)(ΗΣΕΤΕ)$\")\n",
    "        if pattern4.match(w):\n",
    "            fp = pattern4.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "\n",
    "        if pattern3.match(w):\n",
    "            fp = pattern3.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            pattern3 = re.compile(Stemmer.refinedVowels + \"$\")\n",
    "            except8 = re.compile(\n",
    "                r\"(ΟΔ|ΑΙΡ|ΦΟΡ|ΤΑΘ|ΔΙΑΘ|ΣΧ|ΕΝΔ|ΕΥΡ|ΤΙΘ|ΥΠΕΡΘ|ΡΑΘ|ΕΝΘ|ΡΟΘ|ΣΘ|ΠΥΡ|ΑΙΝ|ΣΥΝΔ|ΣΥΝ|ΣΥΝΘ|ΧΩΡ|ΠΟΝ|ΒΡ|ΚΑΘ|ΕΥΘ|ΕΚΘ|ΝΕΤ|ΡΟΝ|ΑΡΚ|ΒΑΡ|ΒΟΛ|ΩΦΕΛ)$\")\n",
    "            except9 = re.compile(\n",
    "                r\"^(ΑΒΑΡ|ΒΕΝ|ΕΝΑΡ|ΑΒΡ|ΑΔ|ΑΘ|ΑΝ|ΑΠΛ|ΒΑΡΟΝ|ΝΤΡ|ΣΚ|ΚΟΠ|ΜΠΟΡ|ΝΙΦ|ΠΑΓ|ΠΑΡΑΚΑΛ|ΣΕΡΠ|ΣΚΕΛ|ΣΥΡΦ|ΤΟΚ|Υ|Δ|ΕΜ|ΘΑΡΡ|Θ)$\")\n",
    "            if (pattern3.match(w)) or (except8.match(w)) or (except9.match(w)):\n",
    "                w = w + \"ΕΤ\"\n",
    "\n",
    "        # Step 5d\n",
    "        pattern = re.compile(r\"^(.+?)(ΟΝΤΑΣ|ΩΝΤΑΣ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except10 = re.compile(r\"^(ΑΡΧ)$\")\n",
    "            except11 = re.compile(r\"(ΚΡΕ)$\")\n",
    "            if except10.match(w):\n",
    "                w = w + \"ΟΝΤ\"\n",
    "            if except11.match(w):\n",
    "                w = w + \"ΩΝΤ\"\n",
    "\n",
    "        # Step 5e\n",
    "        pattern = re.compile(r\"^(.+?)(ΟΜΑΣΤΕ|ΙΟΜΑΣΤΕ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except11 = re.compile(\"^(ΟΝ)$\")\n",
    "            if except11.match(w):\n",
    "                w = w + \"ΟΜΑΣΤ\"\n",
    "\n",
    "        # Step 5f\n",
    "        pattern = re.compile(r\"^(.+?)(ΕΣΤΕ)$\")\n",
    "        pattern2 = re.compile(r\"^(.+?)(ΙΕΣΤΕ)$\")\n",
    "        if pattern2.match(w):\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            pattern2 = re.compile(r\"^(Π|ΑΠ|ΣΥΜΠ|ΑΣΥΜΠ|ΑΚΑΤΑΠ|ΑΜΕΤΑΜΦ)$\")\n",
    "            if pattern2.match(w):\n",
    "                w = w + \"ΙΕΣΤ\"\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except12 = re.compile(r\"^(ΑΛ|ΑΡ|ΕΚΤΕΛ|Ζ|Μ|Ξ|ΠΑΡΑΚΑΛ|ΑΡ|ΠΡΟ|ΝΙΣ)$\")\n",
    "            if except12.match(w):\n",
    "                w = w + \"ΕΣΤ\"\n",
    "\n",
    "        # Step 5g\n",
    "        pattern = re.compile(r\"^(.+?)(ΗΚΑ|ΗΚΕΣ|ΗΚΕ)$\")\n",
    "        pattern2 = re.compile(r\"^(.+?)(ΗΘΗΚΑ|ΗΘΗΚΕΣ|ΗΘΗΚΕ)$\")\n",
    "        if pattern2.match(w):\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except13 = re.compile(r\"(ΣΚΩΛ|ΣΚΟΥΛ|ΝΑΡΘ|ΣΦ|ΟΘ|ΠΙΘ)$\")\n",
    "            except14 = re.compile(r\"^(ΔΙΑΘ|Θ|ΠΑΡΑΚΑΤΑΘ|ΠΡΟΣΘ|ΣΥΝΘ|)$\")\n",
    "            if (except13.match(w)) or (except14.match(w)):\n",
    "                w = w + \"ΗΚ\"\n",
    "\n",
    "        # Step 5h\n",
    "        pattern = re.compile(r\"^(.+?)(ΟΥΣΑ|ΟΥΣΕΣ|ΟΥΣΕ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except15 = re.compile(\n",
    "                r\"^(ΦΑΡΜΑΚ|ΧΑΔ|ΑΓΚ|ΑΝΑΡΡ|ΒΡΟΜ|ΕΚΛΙΠ|ΛΑΜΠΙΔ|ΛΕΧ|Μ|ΠΑΤ|Ρ|Λ|ΜΕΔ|ΜΕΣΑΖ|ΥΠΟΤΕΙΝ|ΑΜ|ΑΙΘ|ΑΝΗΚ|ΔΕΣΠΟΖ|ΕΝΔΙΑΦΕΡ|ΔΕ|ΔΕΥΤΕΡΕΥ|ΚΑΘΑΡΕΥ|ΠΛΕ|ΤΣΑ)$\")\n",
    "            except16 = re.compile(r\"(ΠΟΔΑΡ|ΒΛΕΠ|ΠΑΝΤΑΧ|ΦΡΥΔ|ΜΑΝΤΙΛ|ΜΑΛΛ|ΚΥΜΑΤ|ΛΑΧ|ΛΗΓ|ΦΑΓ|ΟΜ|ΠΡΩΤ)$\")\n",
    "            if (except15.match(w)) or (except16.match(w)):\n",
    "                w = w + \"ΟΥΣ\"\n",
    "\n",
    "        # Step 5i\n",
    "        pattern = re.compile(r\"^(.+?)(ΑΓΑ|ΑΓΕΣ|ΑΓΕ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except17 = re.compile(r\"^(ΨΟΦ|ΝΑΥΛΟΧ)$\")\n",
    "            except20 = re.compile(r\"(ΚΟΛΛ)$\")\n",
    "            except18 = re.compile(\n",
    "                r\"^(ΑΒΑΣΤ|ΠΟΛΥΦ|ΑΔΗΦ|ΠΑΜΦ|Ρ|ΑΣΠ|ΑΦ|ΑΜΑΛ|ΑΜΑΛΛΙ|ΑΝΥΣΤ|ΑΠΕΡ|ΑΣΠΑΡ|ΑΧΑΡ|ΔΕΡΒΕΝ|ΔΡΟΣΟΠ|ΞΕΦ|ΝΕΟΠ|ΝΟΜΟΤ|ΟΛΟΠ|ΟΜΟΤ|ΠΡΟΣΤ|ΠΡΟΣΩΠΟΠ|ΣΥΜΠ|ΣΥΝΤ|Τ|ΥΠΟΤ|ΧΑΡ|ΑΕΙΠ|ΑΙΜΟΣΤ|ΑΝΥΠ|ΑΠΟΤ|ΑΡΤΙΠ|ΔΙΑΤ|ΕΝ|ΕΠΙΤ|ΚΡΟΚΑΛΟΠ|ΣΙΔΗΡΟΠ|Λ|ΝΑΥ|ΟΥΛΑΜ|ΟΥΡ|Π|ΤΡ|Μ)$\")\n",
    "            except19 = re.compile(r\"(ΟΦ|ΠΕΛ|ΧΟΡΤ|ΛΛ|ΣΦ|ΡΠ|ΦΡ|ΠΡ|ΛΟΧ|ΣΜΗΝ)$\")\n",
    "            if (except18.match(w) and except19.match(w)) and not ((except17.match(w)) or (except20.match(w))):\n",
    "                w = w + \"ΑΓ\"\n",
    "\n",
    "        # Step 5j\n",
    "        pattern = re.compile(\"^(.+?)(ΗΣΕ|ΗΣΟΥ|ΗΣΑ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except21 = re.compile(r\"^(Ν|ΧΕΡΣΟΝ|ΔΩΔΕΚΑΝ|ΕΡΗΜΟΝ|ΜΕΓΑΛΟΝ|ΕΠΤΑΝ)$\")\n",
    "            if except21.match(w):\n",
    "                w = w + \"ΗΣ\"\n",
    "\n",
    "        # Step 5k\n",
    "        pattern = re.compile(r\"^(.+?)(ΗΣΤΕ)$\")\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except22 = re.compile(r\"^(ΑΣΒ|ΣΒ|ΑΧΡ|ΧΡ|ΑΠΛ|ΑΕΙΜΝ|ΔΥΣΧΡ|ΕΥΧΡ|ΚΟΙΝΟΧΡ|ΠΑΛΙΜΨ)$\")\n",
    "            if except22.match(w):\n",
    "                w = w + \"ΗΣΤ\"\n",
    "\n",
    "        # Step 5l\n",
    "        pattern = re.compile(\"^(.+?)(ΟΥΝΕ|ΗΣΟΥΝΕ|ΗΘΟΥΝΕ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except23 = re.compile(\"^(Ν|Ρ|ΣΠΙ|ΣΤΡΑΒΟΜΟΥΤΣ|ΚΑΚΟΜΟΥΤΣ|ΕΞΩΝ)$\")\n",
    "            if except23.match(w):\n",
    "                w = w + \"ΟΥΝ\"\n",
    "\n",
    "        # Step 5l\n",
    "        pattern = re.compile(r\"^(.+?)(ΟΥΜΕ|ΗΣΟΥΜΕ|ΗΘΟΥΜΕ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "            test1 = False\n",
    "            except24 = re.compile(r\"^(ΠΑΡΑΣΟΥΣ|Φ|Χ|ΩΡΙΟΠΛ|ΑΖ|ΑΛΛΟΣΟΥΣ|ΑΣΟΥΣ)$\")\n",
    "            if except24.match(w):\n",
    "                w = w + \"ΟΥΜ\"\n",
    "\n",
    "        # Step 6\n",
    "        pattern = re.compile(r\"^(.+?)(ΜΑΤΑ|ΜΑΤΩΝ|ΜΑΤΟΣ)$\")\n",
    "        pattern2 = re.compile(\n",
    "            r\"^(.+?)(Α|ΑΓΑΤΕ|ΑΓΑΝ|ΑΕΙ|ΑΜΑΙ|ΑΝ|ΑΣ|ΑΣΑΙ|ΑΤΑΙ|ΑΩ|Ε|ΕΙ|ΕΙΣ|ΕΙΤΕ|ΕΣΑΙ|ΕΣ|ΕΤΑΙ|Ι|ΙΕΜΑΙ|ΙΕΜΑΣΤΕ|ΙΕΤΑΙ|ΙΕΣΑΙ|ΙΕΣΑΣΤΕ|ΙΟΜΑΣΤΑΝ|ΙΟΜΟΥΝ|ΙΟΜΟΥΝΑ|ΙΟΝΤΑΝ|ΙΟΝΤΟΥΣΑΝ|ΙΟΣΑΣΤΑΝ|ΙΟΣΑΣΤΕ|ΙΟΣΟΥΝ|ΙΟΣΟΥΝΑ|ΙΟΤΑΝ|ΙΟΥΜΑ|ΙΟΥΜΑΣΤΕ|ΙΟΥΝΤΑΙ|ΙΟΥΝΤΑΝ|Η|ΗΔΕΣ|ΗΔΩΝ|ΗΘΕΙ|ΗΘΕΙΣ|ΗΘΕΙΤΕ|ΗΘΗΚΑΤΕ|ΗΘΗΚΑΝ|ΗΘΟΥΝ|ΗΘΩ|ΗΚΑΤΕ|ΗΚΑΝ|ΗΣ|ΗΣΑΝ|ΗΣΑΤΕ|ΗΣΕΙ|ΗΣΕΣ|ΗΣΟΥΝ|ΗΣΩ|Ο|ΟΙ|ΟΜΑΙ|ΟΜΑΣΤΑΝ|ΟΜΟΥΝ|ΟΜΟΥΝΑ|ΟΝΤΑΙ|ΟΝΤΑΝ|ΟΝΤΟΥΣΑΝ|ΟΣ|ΟΣΑΣΤΑΝ|ΟΣΑΣΤΕ|ΟΣΟΥΝ|ΟΣΟΥΝΑ|ΟΤΑΝ|ΟΥ|ΟΥΜΑΙ|ΟΥΜΑΣΤΕ|ΟΥΝ|ΟΥΝΤΑΙ|ΟΥΝΤΑΝ|ΟΥΣ|ΟΥΣΑΝ|ΟΥΣΑΤΕ|Υ|ΥΣ|Ω|ΩΝ)$\")\n",
    "\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem + \"ΜΑ\"\n",
    "\n",
    "        if pattern2.match(w) and test1:\n",
    "            fp = pattern2.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "\n",
    "        # Step 7 (ΠΑΡΑΘΕΤΙΚΑ)\n",
    "        pattern = re.compile(r\"^(.+?)(ΕΣΤΕΡ|ΕΣΤΑΤ|ΟΤΕΡ|ΟΤΑΤ|ΥΤΕΡ|ΥΤΑΤ|ΩΤΕΡ|ΩΤΑΤ)$\")\n",
    "        if pattern.match(w):\n",
    "            fp = pattern.match(w).groups()\n",
    "            stem = fp[0]\n",
    "            w = stem\n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfda2bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.824411Z",
     "iopub.status.busy": "2024-02-05T18:13:00.824130Z",
     "iopub.status.idle": "2024-02-05T18:13:00.858774Z",
     "shell.execute_reply": "2024-02-05T18:13:00.857935Z"
    },
    "papermill": {
     "duration": 0.048058,
     "end_time": "2024-02-05T18:13:00.861174",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.813116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sanitize:\n",
    "    HYPERLINKS_REGEX = r'(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|(www\\.[^ \\s]+)'\n",
    "    HASHTAGS_REGEX = r'#\\w+'\n",
    "    MENTIONS_REGEX = r'@\\w+'\n",
    "\n",
    "    _stopwordsPath = \"Data/Greek-Stopwords.txt\"\n",
    "    greekStopwords = Stopwords(_stopwordsPath)\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean(text, breakSpecials):\n",
    "        \"\"\"\n",
    "        Utility method that tokenizes a text with the option to either break special characters like \"!!!\" into \"!\", \"!\", \"!\" or keep them together.\n",
    "        \"\"\"\n",
    "        def isAcceptable(character):\n",
    "            return character.isalnum() or character == \" \"\n",
    "\n",
    "        cleanedText = \"\"\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            character = text[i]\n",
    "            if character.isalnum() or character == \" \":\n",
    "                cleanedText += character\n",
    "                i += 1\n",
    "            else:\n",
    "                if not breakSpecials:\n",
    "                    newToken = \"\"\n",
    "                    j = i\n",
    "                    while j < len(text) and not isAcceptable(text[j]):\n",
    "                        newToken += text[j]\n",
    "                        j += 1\n",
    "                        if j < len(text) and text[j] != text[j - 1]:\n",
    "                            break\n",
    "\n",
    "                    cleanedText += f\" {newToken} \"\n",
    "                    i = j\n",
    "                else:\n",
    "                    cleanedText += f\" {text[i]} \"\n",
    "                    i += 1\n",
    "\n",
    "        return \" \".join(cleanedText.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def customTokenize(text, breakSpecials):\n",
    "        return Sanitize._clean(text, breakSpecials).split(), []\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def removeAccents(token):\n",
    "        \"\"\"\n",
    "        Utility method that eliminates a token's accents.\n",
    "        \"\"\"\n",
    "        return ''.join(character for character in unicodedata.normalize('NFD', token) if unicodedata.category(character) != 'Mn')\n",
    "\n",
    "    @staticmethod\n",
    "    def removeSpecialCharacters(token):\n",
    "        \"\"\"\n",
    "        Utility method that eliminates a token's special characters.\n",
    "        \"\"\"\n",
    "        refinedToken = \"\"\n",
    "        for character in token:\n",
    "            if character.isalnum():\n",
    "                refinedToken += character\n",
    "        return refinedToken\n",
    "\n",
    "    @staticmethod\n",
    "    def isNumeric(token):\n",
    "        \"\"\"\n",
    "        Utility method that checks if a token is numeric.\n",
    "        A token is considered numeric if it consists only of numbers and its length is not equal to 4, accounting for cases like dates.\n",
    "        \"\"\"\n",
    "        for character in token:\n",
    "            if not character.isdigit():\n",
    "                return False\n",
    "\n",
    "        return True if len(token) != 4 else False\n",
    "\n",
    "    @staticmethod\n",
    "    def refineHashtag(hashtag):\n",
    "        \"\"\"\n",
    "        Utility method to refine a hashtag by:\n",
    "        Capitalizing it.\n",
    "        Eliminating special characters, as hashtags like #EKLOGES2019 and #EKLOGES_2019 signify the same topic.\n",
    "        Removing accents.\n",
    "        \"\"\"\n",
    "        refinedHashtag = hashtag.upper()\n",
    "        refinedHashtag = Sanitize.removeAccents(refinedHashtag)\n",
    "        refinedHashtag = Sanitize.removeSpecialCharacters(refinedHashtag)\n",
    "        return f\"#{refinedHashtag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f50d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.889485Z",
     "iopub.status.busy": "2024-02-05T18:13:00.889215Z",
     "iopub.status.idle": "2024-02-05T18:13:00.914167Z",
     "shell.execute_reply": "2024-02-05T18:13:00.913332Z"
    },
    "papermill": {
     "duration": 0.040875,
     "end_time": "2024-02-05T18:13:00.916342",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.875467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    LSTM = torch.nn.LSTM.__name__\n",
    "    GRU = torch.nn.GRU.__name__\n",
    "    LAST = \"LAST\"\n",
    "    MEAN = \"MEAN\"\n",
    "\n",
    "    def __init__(self, type, input, aggregation, skipConnections, useParty, seed=None):\n",
    "        \"\"\"\n",
    "        Utility class that implements a neural network following a TensorFlow-like structure.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        if seed is not None:\n",
    "            Model.enableDeterminism(seed)\n",
    "        self.__type = type\n",
    "        self.__input = input\n",
    "        self.__aggregation = aggregation\n",
    "        self.__previousOutput = -1\n",
    "        self.__compiled = False\n",
    "        self.__skipConnections = skipConnections\n",
    "        self.__accumulatedInput = 0\n",
    "        self.__rnn = torch.nn.ModuleList()\n",
    "        self.__layers = torch.nn.ModuleList()\n",
    "        self.__shouldEvaluate = False\n",
    "        self.__useParty = useParty\n",
    "\n",
    "    def forward(self, inputs, lengths, parties, links):\n",
    "        x = inputs\n",
    "        \n",
    "        for i in range(len(self.__rnn)):\n",
    "\n",
    "            currentLayer = self.__rnn[i]\n",
    "            \n",
    "            # If the corresponding layer is an RNN layer, then: \n",
    "            # Pack the input for the RNN to ignore padding.\n",
    "            # Run the packed input through the RNN layer.\n",
    "            # Unpack the output.\n",
    "            # In the case of skip connections, combine the output with the CURRENT layer's input by concatenating them. \n",
    "            if isinstance(currentLayer, torch.nn.RNNBase):\n",
    "                packedInput = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "                output, _ = currentLayer(packedInput)\n",
    "                unpackedOutput, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "                \n",
    "                # Ignore the last RNN layer.\n",
    "                if self.__skipConnections and i < len(self.__rnn) - 1:\n",
    "                    x = torch.cat([x, unpackedOutput], dim=2)\n",
    "                else:\n",
    "                    x = unpackedOutput\n",
    "                    \n",
    "            # In case of a Dropout layer.\n",
    "            else:\n",
    "                x = currentLayer(x)\n",
    "        \n",
    "        stateCollector = []\n",
    "        for index, length in enumerate(lengths):\n",
    "            # Calculate and retrieve the element-wise mean of all hidden states for the corresponding sequence.\n",
    "            if self.__aggregation == Model.MEAN:\n",
    "                stateCollector.append(torch.mean(x[index][:length], dim=0))\n",
    "            elif self.__aggregation == Model.LAST:\n",
    "                # Retrieve the final hidden state for the coorresponding sequence.\n",
    "                stateCollector.append(x[index][length - 1])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid aggregation method\")\n",
    "\n",
    "        x = torch.stack(stateCollector)\n",
    "        \n",
    "        links = links.view(-1, 1)\n",
    "        x = torch.cat((x, links), dim=1)\n",
    "        \n",
    "        if self.__useParty:\n",
    "            parties =  parties.view(-1, 1)\n",
    "            x = torch.cat((x, parties), dim=1) \n",
    "        \n",
    "        # Run the RNN output through the remaining layers.\n",
    "        for i in range(len(self.__layers)):\n",
    "            currentLayer = self.__layers[i]\n",
    "            x = currentLayer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def addRNN(self, hiddenSize, bidirectional, dropout=0.0):\n",
    "        if not self.__compiled:\n",
    "            \n",
    "            # If it's the initial RNN layer, set the input size to the dimensions of the word embeddings; otherwise, use the output size of the preceding RNN layer.\n",
    "            inputSize = self.__input if self.__previousOutput == -1 else self.__previousOutput\n",
    "            self.__accumulatedInput += inputSize\n",
    "            \n",
    "            # In case of skip connections to account for previous input.    \n",
    "            if self.__skipConnections:\n",
    "                inputSize = self.__accumulatedInput\n",
    "\n",
    "            if self.__type == Model.LSTM:\n",
    "                self.__rnn.append(torch.nn.LSTM(input_size=inputSize, hidden_size=hiddenSize, bidirectional=bidirectional, num_layers=1, batch_first=True))\n",
    "\n",
    "            elif self.__type == Model.GRU:\n",
    "                self.__rnn.append(torch.nn.GRU(input_size=inputSize, hidden_size=hiddenSize, bidirectional=bidirectional, num_layers=1, batch_first=True))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid RNN type\")\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                self.__shouldEvaluate = True\n",
    "                self.__rnn.append(torch.nn.Dropout(p=dropout))\n",
    "\n",
    "            self.__previousOutput = 2 * hiddenSize if bidirectional else hiddenSize\n",
    "\n",
    "\n",
    "    def addDense(self, neurons):\n",
    "        if len(self.__rnn) > 0 and not self.__compiled:\n",
    "            \n",
    "            if len(self.__layers) == 0:\n",
    "                \n",
    "                # Account for the inclusion of the links feature.\n",
    "                self.__previousOutput += 1\n",
    "                \n",
    "                # Account for the potential inclusion of the parties feature.\n",
    "                if self.__useParty:\n",
    "                    self.__previousOutput += 1\n",
    "                \n",
    "            \n",
    "            self.__layers.append(torch.nn.Linear(self.__previousOutput, neurons))\n",
    "            self.__previousOutput = neurons\n",
    "\n",
    "    def addRelu(self):\n",
    "        if len(self.__rnn) > 0 and not self.__compiled:\n",
    "            self.__layers.append(nn.ReLU())\n",
    "\n",
    "    def addDropout(self, p):\n",
    "        if p > 0.0 and len(self.__rnn) > 0 and not self.__compiled:\n",
    "            self.__layers.append(nn.Dropout(p=p))\n",
    "            self.__shouldEvaluate = True\n",
    "\n",
    "    def compile(self):\n",
    "        self.__compiled = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(layer) for layer in self.__rnn + self.__layers])\n",
    "\n",
    "    def getRNN(self):\n",
    "        return self.__rnn\n",
    "\n",
    "    def getLayers(self):\n",
    "        return self.__layers\n",
    "\n",
    "    def shouldEvaluate(self):\n",
    "        return self.__shouldEvaluate\n",
    "\n",
    "    @staticmethod\n",
    "    def enableDeterminism(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        numpy.random.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215f56ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.939043Z",
     "iopub.status.busy": "2024-02-05T18:13:00.938716Z",
     "iopub.status.idle": "2024-02-05T18:13:00.958672Z",
     "shell.execute_reply": "2024-02-05T18:13:00.957868Z"
    },
    "papermill": {
     "duration": 0.031843,
     "end_time": "2024-02-05T18:13:00.960533",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.928690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "\n",
    "    def __init__(self, model, metricName, patience, delta, maximize):\n",
    "        \"\"\"\n",
    "        A utility class implementing the early stopping concept with parameters for patience and delta.\n",
    "        This class stores a snapshot of the training session by saving the best epoch, the optimal value for the chosen metric\n",
    "        and the labels and predicted labels for both datasets, as the model will be evaluated based on its best epoch.\n",
    "        \"\"\"\n",
    "        self.__metricName = metricName.upper()\n",
    "        self.__maximize = maximize\n",
    "        self.__patience = patience\n",
    "        self.__delta = delta\n",
    "        self.__counter = 0\n",
    "        self.__bestValue = float('-inf') if maximize else float('inf')\n",
    "        self.__bestEpoch = 0\n",
    "        self.__state = copy.deepcopy(model.state_dict())\n",
    "        self.__shouldStop = False\n",
    "        self.__trainingLabels = None\n",
    "        self.__predictedTrainingLabels = None\n",
    "        self.__validationLabels = None\n",
    "        self.__predictedValidationLabels = None\n",
    "\n",
    "    def update(self, value, epoch, model, trainingLabels, predictedTrainingLabels, validationLabels, predictedValidationLabels):\n",
    "        if not self.__maximize:\n",
    "            # If the current value is lower than the best value minus the delta parameter, store all the required information and reset the counter.\n",
    "            if value < self.__bestValue - self.__delta:\n",
    "                self.__bestValue = value\n",
    "                self.__bestEpoch = epoch\n",
    "                self.__state = copy.deepcopy(model.state_dict())\n",
    "                self.__trainingLabels = copy.deepcopy(trainingLabels)\n",
    "                self.__predictedTrainingLabels = copy.deepcopy(predictedTrainingLabels)\n",
    "                self.__validationLabels = copy.deepcopy(validationLabels)\n",
    "                self.__predictedValidationLabels = copy.deepcopy(predictedValidationLabels)\n",
    "                self.__counter = 0\n",
    "            # Otherwise, increment the counter and verify if we have exceeded the patience limit.\n",
    "            else:\n",
    "                self.__counter += 1\n",
    "                if self.__counter >= self.__patience:\n",
    "                    self.__shouldStop = True\n",
    "\n",
    "        else:\n",
    "            # If the current value is greater than the best value plus the delta parameter, store all the required information and reset the counter.\n",
    "            if value > self.__bestValue + self.__delta:\n",
    "                self.__bestValue = value\n",
    "                self.__bestEpoch = epoch\n",
    "                self.__state = copy.deepcopy(model.state_dict())\n",
    "                self.__trainingLabels = copy.deepcopy(trainingLabels)\n",
    "                self.__predictedTrainingLabels = copy.deepcopy(predictedTrainingLabels)\n",
    "                self.__validationLabels = copy.deepcopy(validationLabels)\n",
    "                self.__predictedValidationLabels = copy.deepcopy(predictedValidationLabels)\n",
    "                self.__counter = 0\n",
    "            # Otherwise, increment the counter and verify if we have exceeded the patience limit.\n",
    "            else:\n",
    "                self.__counter += 1\n",
    "                if self.__counter >= self.__patience:\n",
    "                    self.__shouldStop = True\n",
    "\n",
    "    def shouldStop(self):\n",
    "        return self.__shouldStop\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"EarlyStopper:\\n\"\n",
    "                f\"Metric: {self.__metricName}\\n\"\n",
    "                f\"Maximize: {self.__maximize}\\n\"\n",
    "                f\"Value: {self.__bestValue}\\n\"\n",
    "                f\"Epoch: {self.__bestEpoch}\\n\"\n",
    "                f\"Patience: {self.__patience}\\n\"\n",
    "                f\"Delta: {self.__delta}\")\n",
    "\n",
    "    def getMetric(self):\n",
    "        return self.__metricName\n",
    "\n",
    "    def getEpoch(self):\n",
    "        return self.__bestEpoch\n",
    "\n",
    "    def getState(self):\n",
    "        return self.__state\n",
    "\n",
    "    def getTrainingLabels(self):\n",
    "        return self.__trainingLabels, self.__predictedTrainingLabels\n",
    "\n",
    "    def getValidationLabels(self):\n",
    "        return self.__validationLabels, self.__predictedValidationLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ef611d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:00.979662Z",
     "iopub.status.busy": "2024-02-05T18:13:00.979343Z",
     "iopub.status.idle": "2024-02-05T18:13:00.986632Z",
     "shell.execute_reply": "2024-02-05T18:13:00.985783Z"
    },
    "papermill": {
     "duration": 0.018879,
     "end_time": "2024-02-05T18:13:00.988525",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.969646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomScheduler:\n",
    "    def __init__(self, scheduler, epoch=None):\n",
    "        \"\"\"\n",
    "        Utility class implementing a custom scheduler that triggers the decay of the learning rate only after a specified number of epochs.\n",
    "        \"\"\"\n",
    "        self.__scheduler = scheduler\n",
    "        self.__epoch = epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if self.__epoch is not None and epoch >= self.__epoch:\n",
    "            self.__scheduler.step()\n",
    "        else:\n",
    "            self.__scheduler.step()\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Custom Scheduler\\n\"\n",
    "                f\"Epoch: {self.__epoch}\\n\"\n",
    "                f\"Scheduler:\\n\"\n",
    "                f\"gamma: {self.__scheduler.gamma}\\n\"\n",
    "                f\"step: {self.__scheduler.step_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a93096e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:01.007110Z",
     "iopub.status.busy": "2024-02-05T18:13:01.006816Z",
     "iopub.status.idle": "2024-02-05T18:13:01.023471Z",
     "shell.execute_reply": "2024-02-05T18:13:01.022601Z"
    },
    "papermill": {
     "duration": 0.028578,
     "end_time": "2024-02-05T18:13:01.025737",
     "exception": false,
     "start_time": "2024-02-05T18:13:00.997159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitizeDataset(path, isTest):\n",
    "    \"\"\"\n",
    "    A utility function designed to preprocess the given dataset.\n",
    "    \"\"\"\n",
    "    dataframe = pandas.read_csv(path, encoding=\"utf-8\")\n",
    "    tweets = []\n",
    "\n",
    "    hashtagPlaceholder = \"TWEETHASHTAGPLACEHOLDER\"\n",
    "    linkPlaceholder = \"TWEETLINKPLACEHOLDER\"\n",
    "    mentionPlaceholder = \"TWEETMENTIONPLACEHOLDER\"\n",
    "    designatedMentions = {\"@ATSIPRAS\", \"@YANISVAROUFAKIS\", \"@KMITSOTAKIS\", \"@VELOPKY\", \"@ELLINIKILISI\",\n",
    "                          \"@ADONISGEORGIADI\", \"@SKAIGR\", \"@NEADEMOKRATIA\", \"@AVGIONLINE\", \"@ΝΕΑ\"}\n",
    "\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "\n",
    "        tweet = Tweet()\n",
    "\n",
    "        tweet.setID(row['New_ID'])\n",
    "        tweet.setParty(row['Party'])\n",
    "\n",
    "        if not isTest:\n",
    "            tweet.setSentiment(Tweet.classToNumber[row['Sentiment']])\n",
    "\n",
    "\n",
    "        text = row['Text']\n",
    "\n",
    "\n",
    "        # Identify and extract all links from the tweet.\n",
    "        # Store all the links from the tweet.\n",
    "        # Substitute all the identified links in the tweet with a placeholder for links.\n",
    "        # This approach was implemented to retain the links' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        links = re.findall(Sanitize.HYPERLINKS_REGEX, text)\n",
    "        links = [\"\".join(group) for group in links]\n",
    "        tweet.addLinks(links)\n",
    "        replacement = f\" {linkPlaceholder} \"\n",
    "        for link in links:\n",
    "            text = text.replace(link, replacement)\n",
    "\n",
    "        # Identify and extract all mentions from the tweet.\n",
    "        # Store all the mentions from the tweet.\n",
    "        # Substitute all the identified mentions in the tweet with a placeholder for mentions.\n",
    "        # This approach was implemented to retain the mentions' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        mentions = re.findall(Sanitize.MENTIONS_REGEX, text)\n",
    "        tweet.addMentions(mentions)\n",
    "        replacement = f\" {mentionPlaceholder} \"\n",
    "        for mention in mentions:\n",
    "            text = text.replace(mention, replacement)\n",
    "\n",
    "        # Identify and extract all hashtags from the tweet.\n",
    "        # Store all the hashtags from the tweet.\n",
    "        # Substitute all the identified hashtags in the tweet with a placeholder for hashtags.\n",
    "        # This approach was implemented to retain the hashtags' positions during the training of the word embeddings model, acknowledging the importance of context.\n",
    "        hashtags = re.findall(Sanitize.HASHTAGS_REGEX, text)\n",
    "        tweet.addHashtags(hashtags)\n",
    "        replacement = f\" {hashtagPlaceholder} \"\n",
    "        for hashtag in hashtags:\n",
    "            text = text.replace(hashtag, replacement)\n",
    "\n",
    "        # Transform the text of the tweet to uppercase and replace Ν.Δ. with ΝΔ.\n",
    "        text = text.upper()\n",
    "        text = text.replace(\"Ν.Δ.\", \" ΝΔ \")\n",
    "\n",
    "        tweetTokens, lemmatizedTokens = Sanitize.customTokenize(text, breakSpecials=True)\n",
    "        refinedTokens = []\n",
    "        hashtagIndex = 0\n",
    "        mentionIndex = 0\n",
    "\n",
    "        for token in tweetTokens:\n",
    "\n",
    "            # If the corresponding token is a hashtag, refine and append it to the list of tokens.\n",
    "            if token == hashtagPlaceholder:\n",
    "                refinedToken = hashtags[hashtagIndex]\n",
    "                refinedToken = Sanitize.refineHashtag(refinedToken)\n",
    "                refinedTokens.append(refinedToken)\n",
    "                hashtagIndex += 1\n",
    "                continue\n",
    "\n",
    "            # If the corresponding token is a link, ignore it.\n",
    "            if token == linkPlaceholder:\n",
    "                continue\n",
    "\n",
    "            # If the corresponding token is a mention, convert it to uppercase and\n",
    "            # include it in the list of tokens only if it belongs to the specified set of designated mentions.\n",
    "            if token == mentionPlaceholder:\n",
    "                refinedToken = mentions[mentionIndex].upper()\n",
    "                mentionIndex += 1\n",
    "                if refinedToken in designatedMentions:\n",
    "                    refinedTokens.append(refinedToken)\n",
    "                continue\n",
    "\n",
    "            # Eliminate special characters.\n",
    "            if len(Sanitize.removeSpecialCharacters(token)) == 0:\n",
    "                continue\n",
    "\n",
    "            # Eliminate numeric characters.\n",
    "            if Sanitize.isNumeric(token):\n",
    "                continue\n",
    "\n",
    "            # Eliminate accents.\n",
    "            refinedToken = Sanitize.removeAccents(token)\n",
    "\n",
    "            # If the corresponding token is a stopword, ignore it.\n",
    "            if refinedToken in Sanitize.greekStopwords:\n",
    "                continue\n",
    "\n",
    "            # Stem the corresponding token.\n",
    "            refinedToken = Stemmer.stemWord(refinedToken)\n",
    "\n",
    "            if len(refinedToken) > 0:\n",
    "                refinedTokens.append(refinedToken)\n",
    "\n",
    "        # Append the <END> token.\n",
    "        refinedTokens.append(Embeddings.END)\n",
    "        tweet.setText(\" \".join(refinedTokens))\n",
    "\n",
    "        tweets.append(tweet)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb733687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:01.050218Z",
     "iopub.status.busy": "2024-02-05T18:13:01.049852Z",
     "iopub.status.idle": "2024-02-05T18:13:01.847138Z",
     "shell.execute_reply": "2024-02-05T18:13:01.846196Z"
    },
    "papermill": {
     "duration": 0.810852,
     "end_time": "2024-02-05T18:13:01.849606",
     "exception": false,
     "start_time": "2024-02-05T18:13:01.038754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    embeddingsPath = \"Data/Vectors.txt\"\n",
    "    embeddings = Embeddings(embeddingsPath)\n",
    "    partyToNumber = dict()\n",
    "\n",
    "    def __init__(self, tweets, isTrain):\n",
    "        self.__instances = []\n",
    "        self.__labels = []\n",
    "        self.__ids = []\n",
    "        self.__lengths = []\n",
    "        self.__parties = []\n",
    "        self.__links = []\n",
    "        if isTrain:\n",
    "            TweetDataset.__partiesToNumbers(tweets)\n",
    "        self.__transform(tweets)\n",
    "    \n",
    "    def __partiesToNumbers(tweets):\n",
    "        partyIndex = 1\n",
    "        \n",
    "        # Assign each party a unique numerical identifier.\n",
    "        for tweet in tweets:\n",
    "            if tweet.getParty() not in TweetDataset.partyToNumber.keys():\n",
    "                TweetDataset.partyToNumber[tweet.getParty()] = partyIndex\n",
    "                partyIndex += 1\n",
    "        \n",
    "        # Normalize parties' unique numerical identifiers.\n",
    "        partyIndex -= 1\n",
    "        for key in TweetDataset.partyToNumber.keys():\n",
    "            TweetDataset.partyToNumber[key] /= partyIndex\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__instances[index], self.__labels[index], self.__lengths[index], self.__parties[index], self.__links[index]\n",
    "\n",
    "    def getInstances(self):\n",
    "        return self.__instances\n",
    "\n",
    "    def getIDS(self):\n",
    "        return self.__ids\n",
    "\n",
    "    def __transform(self, tweets):\n",
    "\n",
    "        for tweet in tweets:\n",
    "\n",
    "            self.__ids.append(tweet.getID())\n",
    "            \n",
    "            if tweet.getSentiment() is not None:\n",
    "                self.__labels.append(tweet.getSentiment())\n",
    "            \n",
    "            else:\n",
    "                self.__labels.append(-1)\n",
    "                \n",
    "            if tweet.getTotalLinks() > 0:\n",
    "                self.__links.append(1.0)\n",
    "            \n",
    "            else:\n",
    "                self.__links.append(0.0)\n",
    "            \n",
    "            # Include the party of the corresponding tweet or ignore it by setting the value to 0.\n",
    "            if tweet.getParty() in TweetDataset.partyToNumber.keys():\n",
    "                self.__parties.append(TweetDataset.partyToNumber[tweet.getParty()])\n",
    "            else:\n",
    "                self.__parties.append(0.0)\n",
    "            \n",
    "            instance = []\n",
    "            for token in tweet.getText().split():\n",
    "                if token in TweetDataset.embeddings:\n",
    "                    instance.append(TweetDataset.embeddings[token])\n",
    "\n",
    "            self.__lengths.append(len(instance))\n",
    "\n",
    "            instance = torch.tensor(instance, dtype=torch.float32)\n",
    "            self.__instances.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c95f236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:01.869446Z",
     "iopub.status.busy": "2024-02-05T18:13:01.869139Z",
     "iopub.status.idle": "2024-02-05T18:13:01.876624Z",
     "shell.execute_reply": "2024-02-05T18:13:01.875834Z"
    },
    "papermill": {
     "duration": 0.019195,
     "end_time": "2024-02-05T18:13:01.878435",
     "exception": false,
     "start_time": "2024-02-05T18:13:01.859240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handleBatch(batch):\n",
    "    \"\"\"\n",
    "    Utility function that ensures that the batch is padded up to the length of the longest sequence in it. \n",
    "    \"\"\"\n",
    "    instances = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    lengths = [item[2] for item in batch]\n",
    "    parties = [item[3] for item in batch]\n",
    "    links = [item[4] for item in batch]\n",
    "    instances = torch.nn.utils.rnn.pad_sequence(instances, batch_first=True).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    parties = torch.tensor(parties, dtype=torch.float32).to(device)\n",
    "    links = torch.tensor(links, dtype=torch.float32).to(device)\n",
    "\n",
    "    return instances, labels, lengths, parties, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c574d18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:01.896878Z",
     "iopub.status.busy": "2024-02-05T18:13:01.896618Z",
     "iopub.status.idle": "2024-02-05T18:13:44.919657Z",
     "shell.execute_reply": "2024-02-05T18:13:44.918845Z"
    },
    "papermill": {
     "duration": 43.034891,
     "end_time": "2024-02-05T18:13:44.922106",
     "exception": false,
     "start_time": "2024-02-05T18:13:01.887215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingSetPath = \"Data/train_set.csv\"\n",
    "validationSetPath = \"Data/valid_set.csv\"\n",
    "testingSetPath = \"Data/test_set.csv\"\n",
    "\n",
    "trainingTweets = sanitizeDataset(path=trainingSetPath, isTest=False)\n",
    "\n",
    "validationTweets = sanitizeDataset(path=validationSetPath, isTest=False)\n",
    "\n",
    "testingTweets = sanitizeDataset(path=testingSetPath, isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa45404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:44.941609Z",
     "iopub.status.busy": "2024-02-05T18:13:44.941329Z",
     "iopub.status.idle": "2024-02-05T18:13:53.308227Z",
     "shell.execute_reply": "2024-02-05T18:13:53.307413Z"
    },
    "papermill": {
     "duration": 8.379051,
     "end_time": "2024-02-05T18:13:53.310523",
     "exception": false,
     "start_time": "2024-02-05T18:13:44.931472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "trainingDataset = TweetDataset(tweets=trainingTweets, isTrain=True)\n",
    "\n",
    "validationDataset = TweetDataset(tweets=validationTweets, isTrain=False)\n",
    "\n",
    "testingDataset = TweetDataset(tweets=testingTweets, isTrain=False)\n",
    "\n",
    "batchSize = 128\n",
    "trainDataloader = DataLoader(trainingDataset, batch_size=batchSize, shuffle=True, collate_fn=handleBatch)\n",
    "validationDataloader = DataLoader(validationDataset, batch_size=batchSize, shuffle=False, collate_fn=handleBatch)\n",
    "testingDataloader = DataLoader(testingDataset, batch_size=len(testingDataset), shuffle=False, collate_fn=handleBatch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa35515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:53.329843Z",
     "iopub.status.busy": "2024-02-05T18:13:53.329567Z",
     "iopub.status.idle": "2024-02-05T18:13:53.342657Z",
     "shell.execute_reply": "2024-02-05T18:13:53.341828Z"
    },
    "papermill": {
     "duration": 0.024872,
     "end_time": "2024-02-05T18:13:53.344422",
     "exception": false,
     "start_time": "2024-02-05T18:13:53.319550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateModel(predictions, labels, average=\"weighted\"):\n",
    "    \"\"\"\n",
    "    Utility function to evaluate a model's F1, recall, precision and accuracy scores.\n",
    "    \"\"\"\n",
    "    # Convert predictions and labels to NumPy arrays.\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    # Calculate F1, recall, precision, and accuracy.\n",
    "    f1 = f1_score(labels, predictions, average=average)\n",
    "    recall = recall_score(labels, predictions, average=average)\n",
    "    precision = precision_score(labels, predictions, average=average)\n",
    "    accuracy = numpy.sum(predictions == labels) / len(labels)\n",
    "\n",
    "    return f1, recall, precision, accuracy\n",
    "\n",
    "\n",
    "def plotLearningCurve(xAxis, toPlot, xAxisName, yAxisName, logger):\n",
    "    \"\"\"\n",
    "    Utility function to plot the learning curve for training and validation metrics over epochs.\n",
    "    \"\"\"\n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot each array in 'toPlot' with a corresponding label.\n",
    "    for array, label in toPlot:\n",
    "        pyplot.plot(xAxis, array, label=label)\n",
    "\n",
    "    # Set axis labels, legend, and display/store the plot.\n",
    "    pyplot.xlabel(xAxisName)\n",
    "    pyplot.ylabel(yAxisName)\n",
    "    pyplot.legend()\n",
    "    pyplot.grid(True)\n",
    "\n",
    "    # Store the plot if a logger is provided.\n",
    "    if logger is not None:\n",
    "        pyplot.savefig(f\"{logger.directory()}\\\\{yAxisName}.png\")\n",
    "\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def plotConfusionMatrix(labels, predictedLabels, logger, title):\n",
    "    \"\"\"\n",
    "    Utility function to plot and store the confusion matrix based on true and predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = labels.cpu().numpy()\n",
    "    predictedLabels = predictedLabels.cpu().numpy()\n",
    "\n",
    "    # Calculate and display the confusion matrix.\n",
    "    sentimentLabels = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "    confusionMatrix = confusion_matrix(labels, predictedLabels, labels=[0, 1, 2])\n",
    "    confusionMatrixDisplay = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=sentimentLabels)\n",
    "    confusionMatrixDisplay.plot(colorbar=False)\n",
    "\n",
    "    # Store the plot if a logger is provided.\n",
    "    if logger is not None:\n",
    "        pyplot.savefig(f\"{logger.directory()}\\\\{title}.png\")\n",
    "\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def classificationReport(labels, predictedLabels, logger, title):\n",
    "    \"\"\"\n",
    "    Utility function to generate and log a classification report based on true and predicted labels.\n",
    "    \"\"\"\n",
    "    sentimentLabels = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
    "\n",
    "    labels = labels.cpu().numpy()\n",
    "    predictedLabels = predictedLabels.cpu().numpy()\n",
    "\n",
    "    # Generate and log the classification report.\n",
    "    report = classification_report(labels, predictedLabels, target_names=sentimentLabels, labels=[0, 1, 2])\n",
    "\n",
    "    # Store the classification report if a logger is provided.\n",
    "    if logger is not None:\n",
    "        logger.log(report, title=title)\n",
    "\n",
    "    print(title)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0595b676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:53.363533Z",
     "iopub.status.busy": "2024-02-05T18:13:53.363266Z",
     "iopub.status.idle": "2024-02-05T18:13:53.401822Z",
     "shell.execute_reply": "2024-02-05T18:13:53.401012Z"
    },
    "papermill": {
     "duration": 0.050425,
     "end_time": "2024-02-05T18:13:53.403731",
     "exception": false,
     "start_time": "2024-02-05T18:13:53.353306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epochs, earlyStop, logger, clipping):\n",
    "\n",
    "    trainingLoss = []\n",
    "    validationLoss = []\n",
    "    trainingMetrics = []\n",
    "    validationMetrics = []\n",
    "\n",
    "    trainingLabels = None\n",
    "    predictedTrainingLabels = None\n",
    "    validationLabels = None\n",
    "    predictedValidationLabels = None\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch the model to training mode.\n",
    "        model.train()\n",
    "        epochLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        trainingLabels = torch.LongTensor([]).to(device)\n",
    "        predictedTrainingLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        for instances, labels, lengths, parties, links in trainDataloader:\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            trainingLabels = torch.cat((trainingLabels, labels))\n",
    "\n",
    "            # Forward pass.\n",
    "            logits = model(instances, lengths, parties, links)\n",
    "            probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "            predictedTrainingLabels = torch.cat((predictedTrainingLabels, predictedLabels))\n",
    "\n",
    "            # Loss calculation.\n",
    "            loss = criterion(logits, labels)\n",
    "            epochLoss += loss.item()\n",
    "\n",
    "            # Backward pass, gradient clipping, and optimization.\n",
    "            loss.backward()\n",
    "            if clipping > 0.0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "            optimizer.step()\n",
    "\n",
    "        # # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss.\n",
    "        epochLoss /= len(trainDataloader)\n",
    "        trainingLoss.append(epochLoss)\n",
    "\n",
    "        # Assess and store the training metrics.\n",
    "        metrics = evaluateModel(predictions=predictedTrainingLabels, labels=trainingLabels)\n",
    "        trainingMetrics.append(metrics)\n",
    "\n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "        epochLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the validation set.\n",
    "        validationLabels = torch.LongTensor([]).to(device)\n",
    "        predictedValidationLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels, lengths, parties, links in validationDataloader:\n",
    "\n",
    "                validationLabels = torch.cat((validationLabels, labels))\n",
    "\n",
    "                # Forward pass.\n",
    "                logits = model(instances, lengths, parties, links)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedValidationLabels = torch.cat((predictedValidationLabels, predictedLabels))\n",
    "\n",
    "                # Loss calculation.\n",
    "                epochLoss += criterion(logits, labels).item()\n",
    "\n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the validation set, as each batch contributes its own individual loss.\n",
    "        epochLoss /= len(validationDataloader)\n",
    "        validationLoss.append(epochLoss)\n",
    "\n",
    "        # Evaluate and store validation metrics\n",
    "        metrics = evaluateModel(predictions=predictedValidationLabels, labels=validationLabels)\n",
    "        validationMetrics.append(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch)\n",
    "\n",
    "        if logger is not None:\n",
    "            logger.log(f'Epoch {epoch + 1}/{epochs}\\n'\n",
    "                       f'Training Loss: {trainingLoss[-1]:.6f}, Validation Loss: {validationLoss[-1]:.6f}\\n'\n",
    "                       f'Training F1: {trainingMetrics[-1][0]:.6f}, Validation F1: {validationMetrics[-1][0]:.6f}\\n')\n",
    "            logger.line()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}\\n'\n",
    "              f'Training Loss: {trainingLoss[-1]:.6f}, Validation Loss: {validationLoss[-1]:.6f}\\n'\n",
    "              f'Training F1: {trainingMetrics[-1][0]:.6f}, Validation F1: {validationMetrics[-1][0]:.6f}\\n'\n",
    "              f'Training Recall: {trainingMetrics[-1][1]:.6f}, Validation Recall: {validationMetrics[-1][1]:.6f}\\n'\n",
    "              f'Training Precision: {trainingMetrics[-1][2]:.6f}, Validation Precision: {validationMetrics[-1][2]:.6f}\\n'\n",
    "              f'Training Accuracy: {trainingMetrics[-1][3]:.6f}, Validation Accuracy: {validationMetrics[-1][3]:.6f}\\n')\n",
    "\n",
    "        if earlyStop is not None:\n",
    "            if earlyStop.getMetric() == \"F1\":\n",
    "                metric = validationMetrics[-1][0]\n",
    "            elif earlyStop.getMetric() == \"RECALL\":\n",
    "                metric = validationMetrics[-1][1]\n",
    "            elif earlyStop.getMetric() == \"PRECISION\":\n",
    "                metric = validationMetrics[-1][2]\n",
    "            elif earlyStop.getMetric() == \"ACCURACY\":\n",
    "                metric = validationMetrics[-1][2]\n",
    "            else:\n",
    "                metric = validationLoss[-1]\n",
    "\n",
    "            # Update the early stopper.\n",
    "            earlyStop.update(metric, epoch + 1, model, trainingLabels, predictedTrainingLabels, validationLabels, predictedValidationLabels)\n",
    "\n",
    "            if earlyStop.shouldStop():\n",
    "                print(f\"earlyStop:\\n{earlyStop}\")\n",
    "                if logger is not None:\n",
    "                    logger.log(f\"earlyStop:\\n{earlyStop}\")\n",
    "                break\n",
    "\n",
    "    # If an evaluation on the training set is intended after the training session, such as for a Dropout layer.\n",
    "    if model.shouldEvaluate():\n",
    "\n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        evaluationLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        evaluationLabels = torch.LongTensor([]).to(device)\n",
    "        predictedEvaluationLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels, lengths, parties, links in trainDataloader:\n",
    "\n",
    "                evaluationLabels = torch.cat((evaluationLabels, labels))\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(instances, lengths, parties, links)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedEvaluationLabels = torch.cat((predictedEvaluationLabels, predictedLabels))\n",
    "\n",
    "                # Loss calculation\n",
    "                evaluationLoss += criterion(logits, labels).item()\n",
    "\n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss.\n",
    "        evaluationLoss /= len(trainDataloader)\n",
    "\n",
    "        # Assess and store the validation metrics.\n",
    "        evaluationMetrics = evaluateModel(predictions=predictedEvaluationLabels, labels=evaluationLabels)\n",
    "\n",
    "\n",
    "    # If early stopping is applied\n",
    "    if earlyStop is not None:\n",
    "\n",
    "        # The optimal epoch is the one determined by the early stopper.\n",
    "        epochs = earlyStop.getEpoch()\n",
    "\n",
    "        # Retrieve the labels and predicted labels for both datasets at the optimal epoch determined by the early stopper.\n",
    "        trainingLabels, predictedTrainingLabels = earlyStop.getTrainingLabels()\n",
    "        validationLabels, predictedValidationLabels = earlyStop.getValidationLabels()\n",
    "\n",
    "\n",
    "\n",
    "    epochsAxis = [i + 1 for i in range(epochs)]\n",
    "\n",
    "    # Obtain the metrics until reaching the optimal epoch, whether it is determined by the early stopper or the final regular training epoch.\n",
    "    trainingLoss = trainingLoss[:epochs]\n",
    "    validationLoss = validationLoss[:epochs]\n",
    "    trainingMetrics = trainingMetrics[:epochs]\n",
    "    validationMetrics = validationMetrics[:epochs]\n",
    "\n",
    "    plotLearningCurve(epochsAxis, [(validationLoss, 'Validation'), (trainingLoss, 'Training')], 'Epoch', 'Loss', logger)\n",
    "\n",
    "    METRICS = [\"F1\", \"RECALL\", \"PRECISION\", \"ACCURACY\"]\n",
    "    for index, metric in enumerate(METRICS):\n",
    "        tMetric = torch.tensor([m[index] for m in trainingMetrics], dtype=torch.float32)\n",
    "        vMetric = torch.tensor([m[index] for m in validationMetrics], dtype=torch.float32)\n",
    "        plotLearningCurve(epochsAxis, [(vMetric, 'Validation'), (tMetric, 'Training')], 'Epoch', metric, logger)\n",
    "\n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the training set.\n",
    "    if not model.shouldEvaluate():\n",
    "        plotConfusionMatrix(trainingLabels, predictedTrainingLabels, logger, \"Training\")\n",
    "        classificationReport(trainingLabels, predictedTrainingLabels, logger, \"Training\")\n",
    "\n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the training set in case of an AFTER TRAINING EVALUATION.\n",
    "    else:\n",
    "        plotConfusionMatrix(evaluationLabels, predictedEvaluationLabels, logger, \"Training\")\n",
    "        classificationReport(evaluationLabels, predictedEvaluationLabels, logger, \"Training\")\n",
    "\n",
    "    # Compute, diplay, and store the confusion matrix and the classification report associated with the valiation set.\n",
    "    plotConfusionMatrix(validationLabels, predictedValidationLabels, logger, \"Validation\")\n",
    "    classificationReport(validationLabels, predictedValidationLabels, logger, \"Validation\")\n",
    "\n",
    "    # If an additional evaluation on the training set is needed.\n",
    "    if model.shouldEvaluate():\n",
    "        if logger is not None:\n",
    "            logger.log(f\"Training:\\n\"\n",
    "                       f\"Loss: {evaluationLoss:.6f} \"\n",
    "                       f\"F1: {evaluationMetrics[0]:.6f} \"\n",
    "                       f\"Recall: {evaluationMetrics[1]:.6f} \"\n",
    "                       f\"Precision: {evaluationMetrics[2]:.6f} \"\n",
    "                       f\"Accuracy: {evaluationMetrics[3]:.6f}\")\n",
    "\n",
    "        print(f\"Training:\\n\"\n",
    "              f\"Loss: {evaluationLoss:.6f} \"\n",
    "              f\"F1: {evaluationMetrics[0]:.6f} \"\n",
    "              f\"Recall: {evaluationMetrics[1]:.6f} \"\n",
    "              f\"Precision: {evaluationMetrics[2]:.6f} \"\n",
    "              f\"Accuracy: {evaluationMetrics[3]:.6f}\")\n",
    "\n",
    "    else:\n",
    "        if logger is not None:\n",
    "            logger.log(f\"Training:\\n\"\n",
    "                       f\"Loss: {trainingLoss[-1]:.6f} \"\n",
    "                       f\"F1: {trainingMetrics[-1][0]:.6f} \"\n",
    "                       f\"Recall: {trainingMetrics[-1][1]:.6f} \"\n",
    "                       f\"Precision: {trainingMetrics[-1][2]:.6f} \"\n",
    "                       f\"Accuracy: {trainingMetrics[-1][3]:.6f}\")\n",
    "\n",
    "        print(f\"Training:\\n\"\n",
    "              f\"Loss: {trainingLoss[-1]:.6f} \"\n",
    "              f\"F1: {trainingMetrics[-1][0]:.6f} \"\n",
    "              f\"Recall: {trainingMetrics[-1][1]:.6f} \"\n",
    "              f\"Precision: {trainingMetrics[-1][2]:.6f} \"\n",
    "              f\"Accuracy: {trainingMetrics[-1][3]:.6f}\")\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.log(f\"Validation:\\n\"\n",
    "                   f\"Loss: {validationLoss[-1]:.6f} \"\n",
    "                   f\"F1: {validationMetrics[-1][0]:.6f} \"\n",
    "                   f\"Recall: {validationMetrics[-1][1]:.6f} \"\n",
    "                   f\"Precision: {validationMetrics[-1][2]:.6f} \"\n",
    "                   f\"Accuracy: {validationMetrics[-1][3]:.6f}\")\n",
    "\n",
    "    print(f\"Validation:\\n\"\n",
    "          f\"Loss: {validationLoss[-1]:.6f} \"\n",
    "          f\"F1: {validationMetrics[-1][0]:.6f} \"\n",
    "          f\"Recall: {validationMetrics[-1][1]:.6f} \"\n",
    "          f\"Precision: {validationMetrics[-1][2]:.6f} \"\n",
    "          f\"Accuracy: {validationMetrics[-1][3]:.6f}\")\n",
    "    \n",
    "    # Generate predictions on the testing set and store them according to the provided instructions.\n",
    "    testingIDS = testingDataset.getIDS()\n",
    "        \n",
    "    testingLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "    # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        for instances, labels, lengths, parties, links in testingDataloader:\n",
    "\n",
    "            # Forward pass.\n",
    "            logits = model(instances, lengths, parties, links)\n",
    "            probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "            testingLabels = torch.cat((testingLabels, predictedLabels))\n",
    "    \n",
    "    \n",
    "    testingLabels = list(testingLabels.cpu().numpy())\n",
    "    for i in range(len(testingLabels)):\n",
    "        testingLabels[i] = Tweet.numberToClass[testingLabels[i]]\n",
    "        \n",
    "    values = list(zip(testingIDS, testingLabels))\n",
    "    dataframe = pandas.DataFrame(values, columns=['Id', 'Predicted'])\n",
    "    dataframe.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049b21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:13:53.422195Z",
     "iopub.status.busy": "2024-02-05T18:13:53.421953Z",
     "iopub.status.idle": "2024-02-05T18:15:54.619018Z",
     "shell.execute_reply": "2024-02-05T18:15:54.618215Z"
    },
    "papermill": {
     "duration": 121.209093,
     "end_time": "2024-02-05T18:15:54.621433",
     "exception": false,
     "start_time": "2024-02-05T18:13:53.412340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputSize = TweetDataset.embeddings.dimensions()\n",
    "aggregation = Model.MEAN\n",
    "skipConnections = True\n",
    "bidirectional = True\n",
    "type = Model.GRU\n",
    "useParty = True\n",
    "dropout = 0.4975131144780398\n",
    "\n",
    "model = Model(type=type, aggregation=aggregation, skipConnections=skipConnections, input=inputSize, seed=seed, useParty=useParty)\n",
    "model.addRNN(hiddenSize=64, bidirectional=bidirectional, dropout=dropout)\n",
    "model.addRNN(hiddenSize=64, bidirectional=bidirectional, dropout=dropout)\n",
    "model.addRNN(hiddenSize=64, bidirectional=bidirectional, dropout=0.0)\n",
    "model.addDense(neurons=3)\n",
    "model.compile()\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "epochs = 10\n",
    "clipping = 0.40389661952680816\n",
    "learningRate = 9.012698664349845e-05\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "\n",
    "train(model=model,\n",
    "      criterion=criterion,\n",
    "      optimizer=optimizer,\n",
    "      scheduler=None,\n",
    "      epochs=epochs,\n",
    "      earlyStop=None,\n",
    "      logger=None,\n",
    "      clipping=clipping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89de31b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.650486Z",
     "iopub.status.busy": "2024-02-05T18:15:54.650201Z",
     "iopub.status.idle": "2024-02-05T18:15:54.658562Z",
     "shell.execute_reply": "2024-02-05T18:15:54.657742Z"
    },
    "papermill": {
     "duration": 0.025179,
     "end_time": "2024-02-05T18:15:54.660441",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.635262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optunaTrain(model, criterion, optimizer, epochs, trainDataloader, validationDataloader, clipping):\n",
    "    trainingLoss = []\n",
    "    validationLoss = []\n",
    "    trainingMetrics = []\n",
    "    validationMetrics = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch the model to training mode.\n",
    "        model.train()\n",
    "        epochLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        trainingLabels = torch.LongTensor([]).to(device)\n",
    "        predictedTrainingLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        for instances, labels, lengths, parties, links in trainDataloader:\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            trainingLabels = torch.cat((trainingLabels, labels))\n",
    "\n",
    "            # Forward pass.\n",
    "            logits = model(instances, lengths, parties, links)\n",
    "            probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "            predictedTrainingLabels = torch.cat((predictedTrainingLabels, predictedLabels))\n",
    "\n",
    "            # Loss calculation.\n",
    "            loss = criterion(logits, labels)\n",
    "            epochLoss += loss.item()\n",
    "\n",
    "            # Backward pass, gradient clipping and optimization.\n",
    "            loss.backward()\n",
    "            if clipping > 0.0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "            optimizer.step()\n",
    "\n",
    "        # # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss.\n",
    "        epochLoss /= len(trainDataloader)\n",
    "        trainingLoss.append(epochLoss)\n",
    "\n",
    "        # Assess and store the training metrics.\n",
    "        metrics = evaluateModel(predictions=predictedTrainingLabels, labels=trainingLabels)\n",
    "        trainingMetrics.append(metrics)\n",
    "\n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "        epochLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the validation set.\n",
    "        validationLabels = torch.LongTensor([]).to(device)\n",
    "        predictedValidationLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels, lengths, parties, links in validationDataloader:\n",
    "\n",
    "                validationLabels = torch.cat((validationLabels, labels))\n",
    "\n",
    "                # Forward pass.\n",
    "                logits = model(instances, lengths, parties, links)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedValidationLabels = torch.cat((predictedValidationLabels, predictedLabels))\n",
    "\n",
    "                # Loss calculation.\n",
    "                epochLoss += criterion(logits, labels).item()\n",
    "\n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the validation set, as each batch contributes its own individual loss.\n",
    "        epochLoss /= len(validationDataloader)\n",
    "        validationLoss.append(epochLoss)\n",
    "\n",
    "        # Evaluate and store validation metrics\n",
    "        metrics = evaluateModel(predictions=predictedValidationLabels, labels=validationLabels)\n",
    "        validationMetrics.append(metrics)\n",
    "\n",
    "    # If an evaluation on the training set is intended after the training session, such as for a Dropout layer.\n",
    "    if model.shouldEvaluate():\n",
    "\n",
    "        # Switch the model to evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        evaluationLoss = 0.0\n",
    "\n",
    "        # Tensors for holding labels and predicted labels of the training set, considering the possibility of randomization due to the DataLoader shuffle\n",
    "        # argument. These labels will be utilized later to assess the model's performance on the training set.\n",
    "        evaluationLabels = torch.LongTensor([]).to(device)\n",
    "        predictedEvaluationLabels = torch.LongTensor([]).to(device)\n",
    "\n",
    "        # Turn off gradient computation since we are in inference/evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            for instances, labels, lengths, parties, links in trainDataloader:\n",
    "\n",
    "                evaluationLabels = torch.cat((evaluationLabels, labels))\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(instances, lengths, parties, links)\n",
    "                probabilities = nn.functional.softmax(logits, dim=1)\n",
    "                predictedLabels = torch.argmax(probabilities, dim=1)\n",
    "                predictedEvaluationLabels = torch.cat((predictedEvaluationLabels, predictedLabels))\n",
    "\n",
    "                # Loss calculation\n",
    "                evaluationLoss += criterion(logits, labels).item()\n",
    "\n",
    "        # The cumulative epoch loss should be divided by the total number of batches in the training set, as each batch contributes its own individual loss.\n",
    "        evaluationLoss /= len(trainDataloader)\n",
    "\n",
    "        # Assess and store the validation metrics.\n",
    "        evaluationMetrics = evaluateModel(predictions=predictedEvaluationLabels, labels=evaluationLabels)\n",
    "\n",
    "\n",
    "    if not model.shouldEvaluate():\n",
    "        return trainingLoss[-1], validationLoss[-1], trainingMetrics[-1], validationMetrics[-1]\n",
    "    else:\n",
    "        return evaluationLoss, validationLoss[-1], evaluationMetrics, validationMetrics[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d046dc14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.724415Z",
     "iopub.status.busy": "2024-02-05T18:15:54.724173Z",
     "iopub.status.idle": "2024-02-05T18:15:54.730723Z",
     "shell.execute_reply": "2024-02-05T18:15:54.729943Z"
    },
    "papermill": {
     "duration": 0.024618,
     "end_time": "2024-02-05T18:15:54.732572",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.707954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "\n",
    "\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "batchSize = 128\n",
    "trainingDataset = TweetDataset(tweets=trainingTweets, isTrain=True)\n",
    "validationDataset = TweetDataset(tweets=validationTweets, isTrain=False)\n",
    "\n",
    "trainDataloader = DataLoader(trainingDataset, batch_size=batchSize, shuffle=True, collate_fn=handleBatch)\n",
    "validationDataloader = DataLoader(validationDataset, batch_size=batchSize, shuffle=False, collate_fn=handleBatch)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    inputSize = TweetDataset.embeddings.dimensions()\n",
    "    aggregation = trial.suggest_categorical(\"aggregation\", [Model.MEAN, Model.LAST])\n",
    "    skipConnections = trial.suggest_categorical(\"skipConnections\", [True, False])\n",
    "\n",
    "    modelType = trial.suggest_categorical(\"modelType\", [Model.LSTM, Model.GRU])\n",
    "    rnnLayers = trial.suggest_int(\"rnnLayers\", low=1, high=3)\n",
    "    hiddenSize = trial.suggest_categorical(\"hiddenSize\", [16, 32, 64, 128])\n",
    "    dropout = trial.suggest_float(\"dropout\", low=0.0, high=0.5)\n",
    "    useParty = trial.suggest_categorical(\"useParty\", [True, False])\n",
    "\n",
    "    model = Model(type=modelType, aggregation=aggregation, skipConnections=skipConnections, input=inputSize, seed=seed, useParty=useParty)\n",
    "    for i in range(rnnLayers):\n",
    "        dropoutValue = dropout if i < (rnnLayers - 1) else 0.0\n",
    "        model.addRNN(hiddenSize=hiddenSize, bidirectional=True, dropout=dropoutValue)\n",
    "\n",
    "    model.addDense(neurons=3)\n",
    "    model.compile()\n",
    "    model = model.to(device)\n",
    "\n",
    "    epochs = trial.suggest_int(\"epochs\", low=1, high=25)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    learningRate = trial.suggest_float('learningRate', low=0.000001, high=0.0001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    clipping = trial.suggest_float('clipping', 0.0, 1.0)\n",
    "\n",
    "    trainingLoss, validationLoss, trainingMetrics, validationMetrics = optunaTrain(model=model,\n",
    "                                                                                   criterion=criterion,\n",
    "                                                                                   optimizer=optimizer,\n",
    "                                                                                   epochs=epochs,\n",
    "                                                                                   trainDataloader=trainDataloader,\n",
    "                                                                                   validationDataloader=validationDataloader,\n",
    "                                                                                   clipping=clipping)\n",
    "\n",
    "\n",
    "    print(f\"Training Loss: {trainingLoss} - Validation Loss: {validationLoss} - Training F1: {trainingMetrics[0]} - Validation F1: {validationMetrics[0]}\\n\")\n",
    "      \n",
    "    if trainingLoss <= validationLoss:\n",
    "      penalty = abs((trainingMetrics[0] - validationMetrics[0]))\n",
    "      return validationMetrics[0] - penalty\n",
    "\n",
    "    else:\n",
    "      return -1\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=70)\n",
    "bestTrial = study.best_trial\n",
    "bestParams = bestTrial.params\n",
    "bestValue = bestTrial.value\n",
    "\n",
    "print(f\"Best Trial:\\n\")\n",
    "print(f\"Best Value: {bestValue}\")\n",
    "print(f\"Best Parameters: {bestParams})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "215b1921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.760790Z",
     "iopub.status.busy": "2024-02-05T18:15:54.760545Z",
     "iopub.status.idle": "2024-02-05T18:15:54.763961Z",
     "shell.execute_reply": "2024-02-05T18:15:54.763154Z"
    },
    "papermill": {
     "duration": 0.019905,
     "end_time": "2024-02-05T18:15:54.765940",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.746035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plot_optimization_history(study)\n",
    "figure.show()\n",
    "\n",
    "figure = plot_param_importances(study)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d44920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.793822Z",
     "iopub.status.busy": "2024-02-05T18:15:54.793582Z",
     "iopub.status.idle": "2024-02-05T18:15:54.797031Z",
     "shell.execute_reply": "2024-02-05T18:15:54.796270Z"
    },
    "papermill": {
     "duration": 0.019517,
     "end_time": "2024-02-05T18:15:54.798945",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.779428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plot_slice(study, params=[\"clipping\", \"epochs\", \"dropout\"])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b2eec7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.827836Z",
     "iopub.status.busy": "2024-02-05T18:15:54.827568Z",
     "iopub.status.idle": "2024-02-05T18:15:54.830970Z",
     "shell.execute_reply": "2024-02-05T18:15:54.830149Z"
    },
    "papermill": {
     "duration": 0.020395,
     "end_time": "2024-02-05T18:15:54.832852",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.812457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plot_parallel_coordinate(study, params=[\"clipping\", \"epochs\", \"dropout\"])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65b58469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T18:15:54.861154Z",
     "iopub.status.busy": "2024-02-05T18:15:54.860837Z",
     "iopub.status.idle": "2024-02-05T18:15:54.864447Z",
     "shell.execute_reply": "2024-02-05T18:15:54.863589Z"
    },
    "papermill": {
     "duration": 0.019755,
     "end_time": "2024-02-05T18:15:54.866283",
     "exception": false,
     "start_time": "2024-02-05T18:15:54.846528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plot_contour(study, params=[\"clipping\", \"epochs\", \"dropout\"])\n",
    "figure.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7351814,
     "sourceId": 66593,
     "sourceType": "competition"
    },
    {
     "datasetId": 4154708,
     "sourceId": 7186549,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 186.424092,
   "end_time": "2024-02-05T18:15:57.308429",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-05T18:12:50.884337",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
